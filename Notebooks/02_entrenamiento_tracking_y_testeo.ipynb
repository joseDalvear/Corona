{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72083436-7b37-4943-99f1-83b9d2c4ed4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Entrenamiento - Tracking y Testeo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97b4f46b-aaba-4839-8c5d-c502ccd5675a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Importe de librerias**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7c3344a-fb01-423c-a222-f81666b38d71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#modelos\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier\n",
    "#evaluación\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "#estructura\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql import Row\n",
    "#tuning\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "#mlflow\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "from mlflow.tracking import MlflowClient\n",
    "from mlflow.models.signature import ModelSignature\n",
    "from mlflow.types import Schema, ColSpec\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#numpy\n",
    "import numpy as np\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "e86edbbb-ca2f-47b5-9c2e-4c38b7959b96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting hyperopt\n  Downloading hyperopt-0.2.7-py2.py3-none-any.whl.metadata (1.7 kB)\nRequirement already satisfied: numpy in /databricks/python3/lib/python3.12/site-packages (from hyperopt) (2.1.3)\nRequirement already satisfied: scipy in /databricks/python3/lib/python3.12/site-packages (from hyperopt) (1.15.1)\nRequirement already satisfied: six in /usr/lib/python3/dist-packages (from hyperopt) (1.16.0)\nCollecting networkx>=2.2 (from hyperopt)\n  Downloading networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\nCollecting future (from hyperopt)\n  Downloading future-1.0.0-py3-none-any.whl.metadata (4.0 kB)\nCollecting tqdm (from hyperopt)\n  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\nRequirement already satisfied: cloudpickle in /databricks/python3/lib/python3.12/site-packages (from hyperopt) (3.0.0)\nRequirement already satisfied: py4j in /databricks/python3/lib/python3.12/site-packages (from hyperopt) (0.10.9.9)\nDownloading hyperopt-0.2.7-py2.py3-none-any.whl (1.6 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/1.6 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.6/1.6 MB\u001B[0m \u001B[31m11.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading networkx-3.5-py3-none-any.whl (2.0 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/2.0 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.0/2.0 MB\u001B[0m \u001B[31m20.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading future-1.0.0-py3-none-any.whl (491 kB)\nDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\nInstalling collected packages: tqdm, networkx, future, hyperopt\nSuccessfully installed future-1.0.0 hyperopt-0.2.7 networkx-3.5 tqdm-4.67.1\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%python\n",
    "#Instalación de libreria para optimizar hiperparametros con buenas practicas\n",
    "#optimizar la selección de hiperparametros\n",
    "%pip install hyperopt\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73a04af0-1585-4910-84af-53376ab002d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Carga de Datos\n",
    "\n",
    "Estos datos fueron generados en el Notebook \"01_Preparacion_EDA\", los cuales ya se encuentran transformados listos para utilizar en el entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02815bc3-0b1b-403f-b834-36b68800408c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>features</th><th>Churn</th></tr></thead><tbody><tr><td>{\"type\":\"1\",\"size\":null,\"indices\":null,\"values\":[\"1.0\",\"0.0\",\"0.0\",\"1.0\",\"0.0\",\"0.495243504188556\",\"0.09683373562402385\",\"0.34374556297032516\",\"0.4966633536845094\",\"0.3448814425670879\",\"0.43944341899758627\",\"0.4931137299446259\",\"0.3989777083629135\",\"0.39542808462303\",\"0.5501916796819537\",\"0.3357944057929859\"]}</td><td>0</td></tr><tr><td>{\"type\":\"1\",\"size\":null,\"indices\":null,\"values\":[\"0.0\",\"0.0\",\"1.0\",\"0.0\",\"0.0\",\"0.504756495811444\",\"0.48132897912821243\",\"0.34374556297032516\",\"0.2866676132329973\",\"0.43844952435041884\",\"0.3438875479199205\",\"0.4931137299446259\",\"0.3989777083629135\",\"0.39542808462303\",\"0.20914383075394008\",\"0.22887973874769274\"]}</td><td>0</td></tr><tr><td>{\"type\":\"1\",\"size\":null,\"indices\":null,\"values\":[\"0.0\",\"0.0\",\"1.0\",\"1.0\",\"1.0\",\"0.504756495811444\",\"0.48132897912821243\",\"0.34374556297032516\",\"0.2866676132329973\",\"0.3448814425670879\",\"0.43944341899758627\",\"0.4931137299446259\",\"0.3989777083629135\",\"0.39542808462303\",\"0.5501916796819537\",\"0.22887973874769274\"]}</td><td>1</td></tr><tr><td>{\"type\":\"1\",\"size\":null,\"indices\":null,\"values\":[\"0.0\",\"0.0\",\"0.0\",\"0.0\",\"0.0\",\"0.504756495811444\",\"0.09683373562402385\",\"0.34374556297032516\",\"0.2866676132329973\",\"0.43844952435041884\",\"0.3438875479199205\",\"0.2902172369728809\",\"0.3989777083629135\",\"0.39542808462303\",\"0.20914383075394008\",\"0.21922476217520942\"]}</td><td>0</td></tr><tr><td>{\"type\":\"1\",\"size\":null,\"indices\":null,\"values\":[\"0.0\",\"0.0\",\"1.0\",\"1.0\",\"1.0\",\"0.495243504188556\",\"0.48132897912821243\",\"0.4395854039471816\",\"0.4966633536845094\",\"0.43844952435041884\",\"0.43944341899758627\",\"0.4931137299446259\",\"0.3989777083629135\",\"0.39542808462303\",\"0.5501916796819537\",\"0.3357944057929859\"]}</td><td>1</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "{\"type\":\"1\",\"size\":null,\"indices\":null,\"values\":[\"1.0\",\"0.0\",\"0.0\",\"1.0\",\"0.0\",\"0.495243504188556\",\"0.09683373562402385\",\"0.34374556297032516\",\"0.4966633536845094\",\"0.3448814425670879\",\"0.43944341899758627\",\"0.4931137299446259\",\"0.3989777083629135\",\"0.39542808462303\",\"0.5501916796819537\",\"0.3357944057929859\"]}",
         0
        ],
        [
         "{\"type\":\"1\",\"size\":null,\"indices\":null,\"values\":[\"0.0\",\"0.0\",\"1.0\",\"0.0\",\"0.0\",\"0.504756495811444\",\"0.48132897912821243\",\"0.34374556297032516\",\"0.2866676132329973\",\"0.43844952435041884\",\"0.3438875479199205\",\"0.4931137299446259\",\"0.3989777083629135\",\"0.39542808462303\",\"0.20914383075394008\",\"0.22887973874769274\"]}",
         0
        ],
        [
         "{\"type\":\"1\",\"size\":null,\"indices\":null,\"values\":[\"0.0\",\"0.0\",\"1.0\",\"1.0\",\"1.0\",\"0.504756495811444\",\"0.48132897912821243\",\"0.34374556297032516\",\"0.2866676132329973\",\"0.3448814425670879\",\"0.43944341899758627\",\"0.4931137299446259\",\"0.3989777083629135\",\"0.39542808462303\",\"0.5501916796819537\",\"0.22887973874769274\"]}",
         1
        ],
        [
         "{\"type\":\"1\",\"size\":null,\"indices\":null,\"values\":[\"0.0\",\"0.0\",\"0.0\",\"0.0\",\"0.0\",\"0.504756495811444\",\"0.09683373562402385\",\"0.34374556297032516\",\"0.2866676132329973\",\"0.43844952435041884\",\"0.3438875479199205\",\"0.2902172369728809\",\"0.3989777083629135\",\"0.39542808462303\",\"0.20914383075394008\",\"0.21922476217520942\"]}",
         0
        ],
        [
         "{\"type\":\"1\",\"size\":null,\"indices\":null,\"values\":[\"0.0\",\"0.0\",\"1.0\",\"1.0\",\"1.0\",\"0.495243504188556\",\"0.48132897912821243\",\"0.4395854039471816\",\"0.4966633536845094\",\"0.43844952435041884\",\"0.43944341899758627\",\"0.4931137299446259\",\"0.3989777083629135\",\"0.39542808462303\",\"0.5501916796819537\",\"0.3357944057929859\"]}",
         1
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{\"ml_attr\": {\"num_attrs\": 16, \"attrs\": {\"numeric\": [{\"name\": \"Partner\", \"idx\": 0}, {\"name\": \"Dependents\", \"idx\": 1}, {\"name\": \"PhoneService\", \"idx\": 2}, {\"name\": \"PaperlessBilling\", \"idx\": 3}, {\"name\": \"Churn\", \"idx\": 4}, {\"name\": \"gender_freq\", \"idx\": 5}, {\"name\": \"MultipleLines_freq\", \"idx\": 6}, {\"name\": \"InternetService_freq\", \"idx\": 7}, {\"name\": \"OnlineSecurity_freq\", \"idx\": 8}, {\"name\": \"OnlineBackup_freq\", \"idx\": 9}, {\"name\": \"DeviceProtection_freq\", \"idx\": 10}, {\"name\": \"TechSupport_freq\", \"idx\": 11}, {\"name\": \"StreamingTV_freq\", \"idx\": 12}, {\"name\": \"StreamingMovies_freq\", \"idx\": 13}, {\"name\": \"Contract_freq\", \"idx\": 14}, {\"name\": \"PaymentMethod_freq\", \"idx\": 15}]}}}",
         "name": "features",
         "type": "{\"class\":\"org.apache.spark.ml.linalg.VectorUDT\",\"pyClass\":\"pyspark.ml.linalg.VectorUDT\",\"sqlType\":{\"fields\":[{\"metadata\":{},\"name\":\"type\",\"nullable\":false,\"type\":\"byte\"},{\"metadata\":{},\"name\":\"size\",\"nullable\":true,\"type\":\"integer\"},{\"metadata\":{},\"name\":\"indices\",\"nullable\":true,\"type\":{\"containsNull\":false,\"elementType\":\"integer\",\"type\":\"array\"}},{\"metadata\":{},\"name\":\"values\",\"nullable\":true,\"type\":{\"containsNull\":false,\"elementType\":\"double\",\"type\":\"array\"}}],\"type\":\"struct\"},\"type\":\"udt\"}"
        },
        {
         "metadata": "{}",
         "name": "Churn",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_churn_T = spark.read.format(\"delta\").load(\"/Volumes/corona/silver/transform\")\n",
    "display(df_churn_T.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8a44b1d2-8edf-4513-9b5a-6e25fab5d954",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Separación de datos - entrenamiento y evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a76dc8de-04ba-46d4-a76f-a693a77bad74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 5634 - Test: 1409\n"
     ]
    }
   ],
   "source": [
    "#Voy a utilizar el 80% para el entrenamiento y 20% para evaluar el modelo\n",
    "train_df, test_df = df_churn_T.randomSplit([0.8, 0.2], seed=42)\n",
    "print(f\"Train: {train_df.count()} - Test: {test_df.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67a3ceab-914c-4959-8996-1fd07be8c495",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Experimento Mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "16e11a59-5bd4-4e52-a250-9f59d5c0c284",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cb284c4-9d7c-4e51-8771-9b5ca2bf329f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/23 17:14:15 INFO mlflow.tracking.fluent: Experiment with name '/Users/josestunt95@gmail.com/ChurnPrediction' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='dbfs:/databricks/mlflow-tracking/2671492619073786', creation_time=1763918055262, experiment_id='2671492619073786', last_update_time=1763918055262, lifecycle_stage='active', name='/Users/josestunt95@gmail.com/ChurnPrediction', tags={'mlflow.experiment.sourceName': '/Users/josestunt95@gmail.com/ChurnPrediction',\n",
       " 'mlflow.experimentType': 'MLFLOW_EXPERIMENT',\n",
       " 'mlflow.ownerEmail': 'josestunt95@gmail.com',\n",
       " 'mlflow.ownerId': '8415925107621832'}>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_experiment(\"/Users/josestunt95@gmail.com/ChurnPrediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e3c7e82-2e38-4579-8cef-29c8f8f57db8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Regresión Logistica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c99076ca-aad3-46c6-8325-a4fef35be511",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Calculo de metricas\n",
    "def compute_metrics(preds):\n",
    "    \"\"\"\n",
    "    Calcula ACC, F1 y AUC usando Spark SQL.\n",
    "    \"\"\"\n",
    "\n",
    "    preds = preds.withColumn(\"prediction_int\", preds[\"prediction\"].cast(\"int\"))\n",
    "\n",
    "    # Accuracy\n",
    "    acc = preds.filter(preds.Churn == preds.prediction_int).count() / preds.count()\n",
    "\n",
    "    # F1 Score\n",
    "    tp = preds.filter(\"prediction_int = 1 AND Churn = 1\").count()\n",
    "    fp = preds.filter(\"prediction_int = 1 AND Churn = 0\").count()\n",
    "    fn = preds.filter(\"prediction_int = 0 AND Churn = 1\").count()\n",
    "\n",
    "    precision = tp / (tp + fp + 1e-9)\n",
    "    recall = tp / (tp + fn + 1e-9)\n",
    "    f1 = 2 * ((precision * recall) / (precision + recall + 1e-9))\n",
    "\n",
    "    #AUC\n",
    "    auc = evaluator.evaluate(preds)\n",
    "\n",
    "    return auc, acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "368ff993-7a10-470e-9311-358a54a19c3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 0/20 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/23 19:02:11 WARNING mlflow.utils.requirements_utils: Found pyspark version (4.0.0+databricks.connect.17.2.2) contains a local version label (+databricks.connect.17.2.2). MLflow logged a pip requirement for this package as 'pyspark==4.0.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n\n2025/11/23 19:02:13 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /local_disk0/user_tmp_data/spark-4fa5dc13-0dd2-4ab5-8c0f-4e/tmpv_2c6ndj/model, flavor: spark). Fall back to return ['pyspark==4.0.0']. Set logging level to DEBUG to see the full traceback. \n\n2025/11/23 19:02:14 WARNING mlflow.models.model: Failed to validate serving input example {\n  \"inputs\": {\n    \"features_0\": 0.0,\n    \"features_1\": 0.0,\n    \"features_2\": 0.0,\n    \"features_3\": 0.0,\n    \"features_4\": 0.0,\n    \"features_5\": 0.495243504188556,\n    \"features_6\": 0.09683373562402385,\n    \"features_7\": 0.34374556297032516,\n    \"features_8\": 0.2866676132329973,\n    \"features_9\": 0.3448814425670879,\n    \"features_10\": 0.43944341899758627,\n    \"features_11\": 0.2902172369728809,\n    \"features_12\": 0.3989777083629135,\n    \"features_13\": 0.39542808462303,\n    \"features_14\": 0.20914383075394008,\n    \"features_15\": 0.2161010932841119\n  }\n}. Alternatively, you can avoid passing input example and pass model signature instead when logging the model. To ensure the input example is valid prior to serving, please try calling `mlflow.models.validate_serving_input` on the model uri and serving input example. A serving input example can be generated from model input example using `mlflow.models.convert_input_example_to_serving_input` function.\nGot error: UC volume path must be provided to save, log or load SparkML models in Databricks shared or serverless clusters. Specify environment variable 'MLFLOW_DFS_TMP' or 'dfs_tmpdir' argument that uses a UC volume path starting with '/Volumes/...' when saving, logging or loading a model.\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1da580a3be434824a949d1cf12659a01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r  5%|▌         | 1/20 [00:16<05:06, 16.15s/trial, best loss: -1.0]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.000590 seconds\n\nINFO:hyperopt.tpe:TPE using 1/1 trials with best loss -1.000000\n\n2025/11/23 19:02:25 WARNING mlflow.utils.requirements_utils: Found pyspark version (4.0.0+databricks.connect.17.2.2) contains a local version label (+databricks.connect.17.2.2). MLflow logged a pip requirement for this package as 'pyspark==4.0.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n\n2025/11/23 19:02:28 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /local_disk0/user_tmp_data/spark-4fa5dc13-0dd2-4ab5-8c0f-4e/tmp6s9hl5hd/model, flavor: spark). Fall back to return ['pyspark==4.0.0']. Set logging level to DEBUG to see the full traceback. \n\n2025/11/23 19:02:28 WARNING mlflow.models.model: Failed to validate serving input example {\n  \"inputs\": {\n    \"features_0\": 0.0,\n    \"features_1\": 0.0,\n    \"features_2\": 0.0,\n    \"features_3\": 0.0,\n    \"features_4\": 0.0,\n    \"features_5\": 0.495243504188556,\n    \"features_6\": 0.09683373562402385,\n    \"features_7\": 0.34374556297032516,\n    \"features_8\": 0.2866676132329973,\n    \"features_9\": 0.3448814425670879,\n    \"features_10\": 0.43944341899758627,\n    \"features_11\": 0.2902172369728809,\n    \"features_12\": 0.3989777083629135,\n    \"features_13\": 0.39542808462303,\n    \"features_14\": 0.20914383075394008,\n    \"features_15\": 0.2161010932841119\n  }\n}. Alternatively, you can avoid passing input example and pass model signature instead when logging the model. To ensure the input example is valid prior to serving, please try calling `mlflow.models.validate_serving_input` on the model uri and serving input example. A serving input example can be generated from model input example using `mlflow.models.convert_input_example_to_serving_input` function.\nGot error: UC volume path must be provided to save, log or load SparkML models in Databricks shared or serverless clusters. Specify environment variable 'MLFLOW_DFS_TMP' or 'dfs_tmpdir' argument that uses a UC volume path starting with '/Volumes/...' when saving, logging or loading a model.\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b639863ba33e43779585cb249b56b24a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r 10%|█         | 2/20 [00:30<04:27, 14.84s/trial, best loss: -1.0]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.000586 seconds\n\nINFO:hyperopt.tpe:TPE using 2/2 trials with best loss -1.000000\n\n2025/11/23 19:02:39 WARNING mlflow.utils.requirements_utils: Found pyspark version (4.0.0+databricks.connect.17.2.2) contains a local version label (+databricks.connect.17.2.2). MLflow logged a pip requirement for this package as 'pyspark==4.0.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n\n2025/11/23 19:02:41 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /local_disk0/user_tmp_data/spark-4fa5dc13-0dd2-4ab5-8c0f-4e/tmpi164wp2u/model, flavor: spark). Fall back to return ['pyspark==4.0.0']. Set logging level to DEBUG to see the full traceback. \n\n2025/11/23 19:02:41 WARNING mlflow.models.model: Failed to validate serving input example {\n  \"inputs\": {\n    \"features_0\": 0.0,\n    \"features_1\": 0.0,\n    \"features_2\": 0.0,\n    \"features_3\": 0.0,\n    \"features_4\": 0.0,\n    \"features_5\": 0.495243504188556,\n    \"features_6\": 0.09683373562402385,\n    \"features_7\": 0.34374556297032516,\n    \"features_8\": 0.2866676132329973,\n    \"features_9\": 0.3448814425670879,\n    \"features_10\": 0.43944341899758627,\n    \"features_11\": 0.2902172369728809,\n    \"features_12\": 0.3989777083629135,\n    \"features_13\": 0.39542808462303,\n    \"features_14\": 0.20914383075394008,\n    \"features_15\": 0.2161010932841119\n  }\n}. Alternatively, you can avoid passing input example and pass model signature instead when logging the model. To ensure the input example is valid prior to serving, please try calling `mlflow.models.validate_serving_input` on the model uri and serving input example. A serving input example can be generated from model input example using `mlflow.models.convert_input_example_to_serving_input` function.\nGot error: UC volume path must be provided to save, log or load SparkML models in Databricks shared or serverless clusters. Specify environment variable 'MLFLOW_DFS_TMP' or 'dfs_tmpdir' argument that uses a UC volume path starting with '/Volumes/...' when saving, logging or loading a model.\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fbcfae12182433790ff5080d1d864b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r 15%|█▌        | 3/20 [00:43<04:04, 14.38s/trial, best loss: -1.0]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.000626 seconds\n\nINFO:hyperopt.tpe:TPE using 3/3 trials with best loss -1.000000\n\n2025/11/23 19:02:52 WARNING mlflow.utils.requirements_utils: Found pyspark version (4.0.0+databricks.connect.17.2.2) contains a local version label (+databricks.connect.17.2.2). MLflow logged a pip requirement for this package as 'pyspark==4.0.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n\n2025/11/23 19:02:55 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /local_disk0/user_tmp_data/spark-4fa5dc13-0dd2-4ab5-8c0f-4e/tmpx9aah2ev/model, flavor: spark). Fall back to return ['pyspark==4.0.0']. Set logging level to DEBUG to see the full traceback. \n\n2025/11/23 19:02:55 WARNING mlflow.models.model: Failed to validate serving input example {\n  \"inputs\": {\n    \"features_0\": 0.0,\n    \"features_1\": 0.0,\n    \"features_2\": 0.0,\n    \"features_3\": 0.0,\n    \"features_4\": 0.0,\n    \"features_5\": 0.495243504188556,\n    \"features_6\": 0.09683373562402385,\n    \"features_7\": 0.34374556297032516,\n    \"features_8\": 0.2866676132329973,\n    \"features_9\": 0.3448814425670879,\n    \"features_10\": 0.43944341899758627,\n    \"features_11\": 0.2902172369728809,\n    \"features_12\": 0.3989777083629135,\n    \"features_13\": 0.39542808462303,\n    \"features_14\": 0.20914383075394008,\n    \"features_15\": 0.2161010932841119\n  }\n}. Alternatively, you can avoid passing input example and pass model signature instead when logging the model. To ensure the input example is valid prior to serving, please try calling `mlflow.models.validate_serving_input` on the model uri and serving input example. A serving input example can be generated from model input example using `mlflow.models.convert_input_example_to_serving_input` function.\nGot error: UC volume path must be provided to save, log or load SparkML models in Databricks shared or serverless clusters. Specify environment variable 'MLFLOW_DFS_TMP' or 'dfs_tmpdir' argument that uses a UC volume path starting with '/Volumes/...' when saving, logging or loading a model.\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a2a130d463a4ba0baf25849f7d70a1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r 20%|██        | 4/20 [00:57<03:44, 14.03s/trial, best loss: -1.0]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.000603 seconds\n\nINFO:hyperopt.tpe:TPE using 4/4 trials with best loss -1.000000\n\n2025/11/23 19:03:06 WARNING mlflow.utils.requirements_utils: Found pyspark version (4.0.0+databricks.connect.17.2.2) contains a local version label (+databricks.connect.17.2.2). MLflow logged a pip requirement for this package as 'pyspark==4.0.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n\n2025/11/23 19:03:08 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /local_disk0/user_tmp_data/spark-4fa5dc13-0dd2-4ab5-8c0f-4e/tmp0_s846zx/model, flavor: spark). Fall back to return ['pyspark==4.0.0']. Set logging level to DEBUG to see the full traceback. \n\n2025/11/23 19:03:08 WARNING mlflow.models.model: Failed to validate serving input example {\n  \"inputs\": {\n    \"features_0\": 0.0,\n    \"features_1\": 0.0,\n    \"features_2\": 0.0,\n    \"features_3\": 0.0,\n    \"features_4\": 0.0,\n    \"features_5\": 0.495243504188556,\n    \"features_6\": 0.09683373562402385,\n    \"features_7\": 0.34374556297032516,\n    \"features_8\": 0.2866676132329973,\n    \"features_9\": 0.3448814425670879,\n    \"features_10\": 0.43944341899758627,\n    \"features_11\": 0.2902172369728809,\n    \"features_12\": 0.3989777083629135,\n    \"features_13\": 0.39542808462303,\n    \"features_14\": 0.20914383075394008,\n    \"features_15\": 0.2161010932841119\n  }\n}. Alternatively, you can avoid passing input example and pass model signature instead when logging the model. To ensure the input example is valid prior to serving, please try calling `mlflow.models.validate_serving_input` on the model uri and serving input example. A serving input example can be generated from model input example using `mlflow.models.convert_input_example_to_serving_input` function.\nGot error: UC volume path must be provided to save, log or load SparkML models in Databricks shared or serverless clusters. Specify environment variable 'MLFLOW_DFS_TMP' or 'dfs_tmpdir' argument that uses a UC volume path starting with '/Volumes/...' when saving, logging or loading a model.\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "880b52c27eb1402fb6f5502e0d8c1696",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r 25%|██▌       | 5/20 [01:10<03:27, 13.80s/trial, best loss: -1.0]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.000551 seconds\n\nINFO:hyperopt.tpe:TPE using 5/5 trials with best loss -1.000000\n\n2025/11/23 19:03:21 WARNING mlflow.utils.requirements_utils: Found pyspark version (4.0.0+databricks.connect.17.2.2) contains a local version label (+databricks.connect.17.2.2). MLflow logged a pip requirement for this package as 'pyspark==4.0.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n\n2025/11/23 19:03:24 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /local_disk0/user_tmp_data/spark-4fa5dc13-0dd2-4ab5-8c0f-4e/tmpq16ea5sr/model, flavor: spark). Fall back to return ['pyspark==4.0.0']. Set logging level to DEBUG to see the full traceback. \n\n2025/11/23 19:03:24 WARNING mlflow.models.model: Failed to validate serving input example {\n  \"inputs\": {\n    \"features_0\": 0.0,\n    \"features_1\": 0.0,\n    \"features_2\": 0.0,\n    \"features_3\": 0.0,\n    \"features_4\": 0.0,\n    \"features_5\": 0.495243504188556,\n    \"features_6\": 0.09683373562402385,\n    \"features_7\": 0.34374556297032516,\n    \"features_8\": 0.2866676132329973,\n    \"features_9\": 0.3448814425670879,\n    \"features_10\": 0.43944341899758627,\n    \"features_11\": 0.2902172369728809,\n    \"features_12\": 0.3989777083629135,\n    \"features_13\": 0.39542808462303,\n    \"features_14\": 0.20914383075394008,\n    \"features_15\": 0.2161010932841119\n  }\n}. Alternatively, you can avoid passing input example and pass model signature instead when logging the model. To ensure the input example is valid prior to serving, please try calling `mlflow.models.validate_serving_input` on the model uri and serving input example. A serving input example can be generated from model input example using `mlflow.models.convert_input_example_to_serving_input` function.\nGot error: UC volume path must be provided to save, log or load SparkML models in Databricks shared or serverless clusters. Specify environment variable 'MLFLOW_DFS_TMP' or 'dfs_tmpdir' argument that uses a UC volume path starting with '/Volumes/...' when saving, logging or loading a model.\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a648122d8dca4c8e92506860318eef90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r 30%|███       | 6/20 [01:26<03:21, 14.38s/trial, best loss: -1.0]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.000556 seconds\n\nINFO:hyperopt.tpe:TPE using 6/6 trials with best loss -1.000000\n\n2025/11/23 19:03:35 WARNING mlflow.utils.requirements_utils: Found pyspark version (4.0.0+databricks.connect.17.2.2) contains a local version label (+databricks.connect.17.2.2). MLflow logged a pip requirement for this package as 'pyspark==4.0.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n\n2025/11/23 19:03:38 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /local_disk0/user_tmp_data/spark-4fa5dc13-0dd2-4ab5-8c0f-4e/tmp9d31p15m/model, flavor: spark). Fall back to return ['pyspark==4.0.0']. Set logging level to DEBUG to see the full traceback. \n\n2025/11/23 19:03:38 WARNING mlflow.models.model: Failed to validate serving input example {\n  \"inputs\": {\n    \"features_0\": 0.0,\n    \"features_1\": 0.0,\n    \"features_2\": 0.0,\n    \"features_3\": 0.0,\n    \"features_4\": 0.0,\n    \"features_5\": 0.495243504188556,\n    \"features_6\": 0.09683373562402385,\n    \"features_7\": 0.34374556297032516,\n    \"features_8\": 0.2866676132329973,\n    \"features_9\": 0.3448814425670879,\n    \"features_10\": 0.43944341899758627,\n    \"features_11\": 0.2902172369728809,\n    \"features_12\": 0.3989777083629135,\n    \"features_13\": 0.39542808462303,\n    \"features_14\": 0.20914383075394008,\n    \"features_15\": 0.2161010932841119\n  }\n}. Alternatively, you can avoid passing input example and pass model signature instead when logging the model. To ensure the input example is valid prior to serving, please try calling `mlflow.models.validate_serving_input` on the model uri and serving input example. A serving input example can be generated from model input example using `mlflow.models.convert_input_example_to_serving_input` function.\nGot error: UC volume path must be provided to save, log or load SparkML models in Databricks shared or serverless clusters. Specify environment variable 'MLFLOW_DFS_TMP' or 'dfs_tmpdir' argument that uses a UC volume path starting with '/Volumes/...' when saving, logging or loading a model.\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94b8e1a13c504dd7ba83a4bd803e78db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r 35%|███▌      | 7/20 [01:40<03:04, 14.19s/trial, best loss: -1.0]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.000659 seconds\n\nINFO:hyperopt.tpe:TPE using 7/7 trials with best loss -1.000000\n\n2025/11/23 19:03:49 WARNING mlflow.utils.requirements_utils: Found pyspark version (4.0.0+databricks.connect.17.2.2) contains a local version label (+databricks.connect.17.2.2). MLflow logged a pip requirement for this package as 'pyspark==4.0.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n\n2025/11/23 19:03:52 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /local_disk0/user_tmp_data/spark-4fa5dc13-0dd2-4ab5-8c0f-4e/tmp8kx3ecw3/model, flavor: spark). Fall back to return ['pyspark==4.0.0']. Set logging level to DEBUG to see the full traceback. \n\n2025/11/23 19:03:52 WARNING mlflow.models.model: Failed to validate serving input example {\n  \"inputs\": {\n    \"features_0\": 0.0,\n    \"features_1\": 0.0,\n    \"features_2\": 0.0,\n    \"features_3\": 0.0,\n    \"features_4\": 0.0,\n    \"features_5\": 0.495243504188556,\n    \"features_6\": 0.09683373562402385,\n    \"features_7\": 0.34374556297032516,\n    \"features_8\": 0.2866676132329973,\n    \"features_9\": 0.3448814425670879,\n    \"features_10\": 0.43944341899758627,\n    \"features_11\": 0.2902172369728809,\n    \"features_12\": 0.3989777083629135,\n    \"features_13\": 0.39542808462303,\n    \"features_14\": 0.20914383075394008,\n    \"features_15\": 0.2161010932841119\n  }\n}. Alternatively, you can avoid passing input example and pass model signature instead when logging the model. To ensure the input example is valid prior to serving, please try calling `mlflow.models.validate_serving_input` on the model uri and serving input example. A serving input example can be generated from model input example using `mlflow.models.convert_input_example_to_serving_input` function.\nGot error: UC volume path must be provided to save, log or load SparkML models in Databricks shared or serverless clusters. Specify environment variable 'MLFLOW_DFS_TMP' or 'dfs_tmpdir' argument that uses a UC volume path starting with '/Volumes/...' when saving, logging or loading a model.\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ece1d623fe2462ba4e0222cc3839d79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r 40%|████      | 8/20 [01:54<02:50, 14.17s/trial, best loss: -1.0]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.000579 seconds\n\nINFO:hyperopt.tpe:TPE using 8/8 trials with best loss -1.000000\n\n2025/11/23 19:04:04 WARNING mlflow.utils.requirements_utils: Found pyspark version (4.0.0+databricks.connect.17.2.2) contains a local version label (+databricks.connect.17.2.2). MLflow logged a pip requirement for this package as 'pyspark==4.0.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n\n2025/11/23 19:04:06 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /local_disk0/user_tmp_data/spark-4fa5dc13-0dd2-4ab5-8c0f-4e/tmp5ku5wbqv/model, flavor: spark). Fall back to return ['pyspark==4.0.0']. Set logging level to DEBUG to see the full traceback. \n\n2025/11/23 19:04:06 WARNING mlflow.models.model: Failed to validate serving input example {\n  \"inputs\": {\n    \"features_0\": 0.0,\n    \"features_1\": 0.0,\n    \"features_2\": 0.0,\n    \"features_3\": 0.0,\n    \"features_4\": 0.0,\n    \"features_5\": 0.495243504188556,\n    \"features_6\": 0.09683373562402385,\n    \"features_7\": 0.34374556297032516,\n    \"features_8\": 0.2866676132329973,\n    \"features_9\": 0.3448814425670879,\n    \"features_10\": 0.43944341899758627,\n    \"features_11\": 0.2902172369728809,\n    \"features_12\": 0.3989777083629135,\n    \"features_13\": 0.39542808462303,\n    \"features_14\": 0.20914383075394008,\n    \"features_15\": 0.2161010932841119\n  }\n}. Alternatively, you can avoid passing input example and pass model signature instead when logging the model. To ensure the input example is valid prior to serving, please try calling `mlflow.models.validate_serving_input` on the model uri and serving input example. A serving input example can be generated from model input example using `mlflow.models.convert_input_example_to_serving_input` function.\nGot error: UC volume path must be provided to save, log or load SparkML models in Databricks shared or serverless clusters. Specify environment variable 'MLFLOW_DFS_TMP' or 'dfs_tmpdir' argument that uses a UC volume path starting with '/Volumes/...' when saving, logging or loading a model.\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f26d9233d79e44c584cd1f1373cbcc49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r 45%|████▌     | 9/20 [02:08<02:36, 14.20s/trial, best loss: -1.0]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.000571 seconds\n\nINFO:hyperopt.tpe:TPE using 9/9 trials with best loss -1.000000\n\n2025/11/23 19:04:18 WARNING mlflow.utils.requirements_utils: Found pyspark version (4.0.0+databricks.connect.17.2.2) contains a local version label (+databricks.connect.17.2.2). MLflow logged a pip requirement for this package as 'pyspark==4.0.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n\n2025/11/23 19:04:20 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /local_disk0/user_tmp_data/spark-4fa5dc13-0dd2-4ab5-8c0f-4e/tmps2upafth/model, flavor: spark). Fall back to return ['pyspark==4.0.0']. Set logging level to DEBUG to see the full traceback. \n\n2025/11/23 19:04:20 WARNING mlflow.models.model: Failed to validate serving input example {\n  \"inputs\": {\n    \"features_0\": 0.0,\n    \"features_1\": 0.0,\n    \"features_2\": 0.0,\n    \"features_3\": 0.0,\n    \"features_4\": 0.0,\n    \"features_5\": 0.495243504188556,\n    \"features_6\": 0.09683373562402385,\n    \"features_7\": 0.34374556297032516,\n    \"features_8\": 0.2866676132329973,\n    \"features_9\": 0.3448814425670879,\n    \"features_10\": 0.43944341899758627,\n    \"features_11\": 0.2902172369728809,\n    \"features_12\": 0.3989777083629135,\n    \"features_13\": 0.39542808462303,\n    \"features_14\": 0.20914383075394008,\n    \"features_15\": 0.2161010932841119\n  }\n}. Alternatively, you can avoid passing input example and pass model signature instead when logging the model. To ensure the input example is valid prior to serving, please try calling `mlflow.models.validate_serving_input` on the model uri and serving input example. A serving input example can be generated from model input example using `mlflow.models.convert_input_example_to_serving_input` function.\nGot error: UC volume path must be provided to save, log or load SparkML models in Databricks shared or serverless clusters. Specify environment variable 'MLFLOW_DFS_TMP' or 'dfs_tmpdir' argument that uses a UC volume path starting with '/Volumes/...' when saving, logging or loading a model.\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dc6ff9806314bd39282b8ba537af706",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r 50%|█████     | 10/20 [02:22<02:21, 14.13s/trial, best loss: -1.0]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.000558 seconds\n\nINFO:hyperopt.tpe:TPE using 10/10 trials with best loss -1.000000\n\n2025/11/23 19:04:31 WARNING mlflow.utils.requirements_utils: Found pyspark version (4.0.0+databricks.connect.17.2.2) contains a local version label (+databricks.connect.17.2.2). MLflow logged a pip requirement for this package as 'pyspark==4.0.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n\n2025/11/23 19:04:34 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /local_disk0/user_tmp_data/spark-4fa5dc13-0dd2-4ab5-8c0f-4e/tmpcfh4vgkx/model, flavor: spark). Fall back to return ['pyspark==4.0.0']. Set logging level to DEBUG to see the full traceback. \n\n2025/11/23 19:04:34 WARNING mlflow.models.model: Failed to validate serving input example {\n  \"inputs\": {\n    \"features_0\": 0.0,\n    \"features_1\": 0.0,\n    \"features_2\": 0.0,\n    \"features_3\": 0.0,\n    \"features_4\": 0.0,\n    \"features_5\": 0.495243504188556,\n    \"features_6\": 0.09683373562402385,\n    \"features_7\": 0.34374556297032516,\n    \"features_8\": 0.2866676132329973,\n    \"features_9\": 0.3448814425670879,\n    \"features_10\": 0.43944341899758627,\n    \"features_11\": 0.2902172369728809,\n    \"features_12\": 0.3989777083629135,\n    \"features_13\": 0.39542808462303,\n    \"features_14\": 0.20914383075394008,\n    \"features_15\": 0.2161010932841119\n  }\n}. Alternatively, you can avoid passing input example and pass model signature instead when logging the model. To ensure the input example is valid prior to serving, please try calling `mlflow.models.validate_serving_input` on the model uri and serving input example. A serving input example can be generated from model input example using `mlflow.models.convert_input_example_to_serving_input` function.\nGot error: UC volume path must be provided to save, log or load SparkML models in Databricks shared or serverless clusters. Specify environment variable 'MLFLOW_DFS_TMP' or 'dfs_tmpdir' argument that uses a UC volume path starting with '/Volumes/...' when saving, logging or loading a model.\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8cb554e02f2432891e88caf7eebbf5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r 55%|█████▌    | 11/20 [02:36<02:05, 13.96s/trial, best loss: -1.0]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.000581 seconds\n\nINFO:hyperopt.tpe:TPE using 11/11 trials with best loss -1.000000\n\n2025/11/23 19:04:46 WARNING mlflow.utils.requirements_utils: Found pyspark version (4.0.0+databricks.connect.17.2.2) contains a local version label (+databricks.connect.17.2.2). MLflow logged a pip requirement for this package as 'pyspark==4.0.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n\n2025/11/23 19:04:49 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /local_disk0/user_tmp_data/spark-4fa5dc13-0dd2-4ab5-8c0f-4e/tmp9mhyksgc/model, flavor: spark). Fall back to return ['pyspark==4.0.0']. Set logging level to DEBUG to see the full traceback. \n\n2025/11/23 19:04:49 WARNING mlflow.models.model: Failed to validate serving input example {\n  \"inputs\": {\n    \"features_0\": 0.0,\n    \"features_1\": 0.0,\n    \"features_2\": 0.0,\n    \"features_3\": 0.0,\n    \"features_4\": 0.0,\n    \"features_5\": 0.495243504188556,\n    \"features_6\": 0.09683373562402385,\n    \"features_7\": 0.34374556297032516,\n    \"features_8\": 0.2866676132329973,\n    \"features_9\": 0.3448814425670879,\n    \"features_10\": 0.43944341899758627,\n    \"features_11\": 0.2902172369728809,\n    \"features_12\": 0.3989777083629135,\n    \"features_13\": 0.39542808462303,\n    \"features_14\": 0.20914383075394008,\n    \"features_15\": 0.2161010932841119\n  }\n}. Alternatively, you can avoid passing input example and pass model signature instead when logging the model. To ensure the input example is valid prior to serving, please try calling `mlflow.models.validate_serving_input` on the model uri and serving input example. A serving input example can be generated from model input example using `mlflow.models.convert_input_example_to_serving_input` function.\nGot error: UC volume path must be provided to save, log or load SparkML models in Databricks shared or serverless clusters. Specify environment variable 'MLFLOW_DFS_TMP' or 'dfs_tmpdir' argument that uses a UC volume path starting with '/Volumes/...' when saving, logging or loading a model.\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb6a55f6538e4de49adb3930ec0665ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r 60%|██████    | 12/20 [02:51<01:54, 14.30s/trial, best loss: -1.0]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.000557 seconds\n\nINFO:hyperopt.tpe:TPE using 12/12 trials with best loss -1.000000\n\n2025/11/23 19:05:01 WARNING mlflow.utils.requirements_utils: Found pyspark version (4.0.0+databricks.connect.17.2.2) contains a local version label (+databricks.connect.17.2.2). MLflow logged a pip requirement for this package as 'pyspark==4.0.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n\n2025/11/23 19:05:03 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /local_disk0/user_tmp_data/spark-4fa5dc13-0dd2-4ab5-8c0f-4e/tmp63aiuktr/model, flavor: spark). Fall back to return ['pyspark==4.0.0']. Set logging level to DEBUG to see the full traceback. \n\n2025/11/23 19:05:03 WARNING mlflow.models.model: Failed to validate serving input example {\n  \"inputs\": {\n    \"features_0\": 0.0,\n    \"features_1\": 0.0,\n    \"features_2\": 0.0,\n    \"features_3\": 0.0,\n    \"features_4\": 0.0,\n    \"features_5\": 0.495243504188556,\n    \"features_6\": 0.09683373562402385,\n    \"features_7\": 0.34374556297032516,\n    \"features_8\": 0.2866676132329973,\n    \"features_9\": 0.3448814425670879,\n    \"features_10\": 0.43944341899758627,\n    \"features_11\": 0.2902172369728809,\n    \"features_12\": 0.3989777083629135,\n    \"features_13\": 0.39542808462303,\n    \"features_14\": 0.20914383075394008,\n    \"features_15\": 0.2161010932841119\n  }\n}. Alternatively, you can avoid passing input example and pass model signature instead when logging the model. To ensure the input example is valid prior to serving, please try calling `mlflow.models.validate_serving_input` on the model uri and serving input example. A serving input example can be generated from model input example using `mlflow.models.convert_input_example_to_serving_input` function.\nGot error: UC volume path must be provided to save, log or load SparkML models in Databricks shared or serverless clusters. Specify environment variable 'MLFLOW_DFS_TMP' or 'dfs_tmpdir' argument that uses a UC volume path starting with '/Volumes/...' when saving, logging or loading a model.\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9d198edfb9f4d659633232f49492ae1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r 65%|██████▌   | 13/20 [03:05<01:40, 14.29s/trial, best loss: -1.0]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.000586 seconds\n\nINFO:hyperopt.tpe:TPE using 13/13 trials with best loss -1.000000\n\n2025/11/23 19:05:15 WARNING mlflow.utils.requirements_utils: Found pyspark version (4.0.0+databricks.connect.17.2.2) contains a local version label (+databricks.connect.17.2.2). MLflow logged a pip requirement for this package as 'pyspark==4.0.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n\n2025/11/23 19:05:17 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /local_disk0/user_tmp_data/spark-4fa5dc13-0dd2-4ab5-8c0f-4e/tmp0hascd7d/model, flavor: spark). Fall back to return ['pyspark==4.0.0']. Set logging level to DEBUG to see the full traceback. \n\n2025/11/23 19:05:17 WARNING mlflow.models.model: Failed to validate serving input example {\n  \"inputs\": {\n    \"features_0\": 0.0,\n    \"features_1\": 0.0,\n    \"features_2\": 0.0,\n    \"features_3\": 0.0,\n    \"features_4\": 0.0,\n    \"features_5\": 0.495243504188556,\n    \"features_6\": 0.09683373562402385,\n    \"features_7\": 0.34374556297032516,\n    \"features_8\": 0.2866676132329973,\n    \"features_9\": 0.3448814425670879,\n    \"features_10\": 0.43944341899758627,\n    \"features_11\": 0.2902172369728809,\n    \"features_12\": 0.3989777083629135,\n    \"features_13\": 0.39542808462303,\n    \"features_14\": 0.20914383075394008,\n    \"features_15\": 0.2161010932841119\n  }\n}. Alternatively, you can avoid passing input example and pass model signature instead when logging the model. To ensure the input example is valid prior to serving, please try calling `mlflow.models.validate_serving_input` on the model uri and serving input example. A serving input example can be generated from model input example using `mlflow.models.convert_input_example_to_serving_input` function.\nGot error: UC volume path must be provided to save, log or load SparkML models in Databricks shared or serverless clusters. Specify environment variable 'MLFLOW_DFS_TMP' or 'dfs_tmpdir' argument that uses a UC volume path starting with '/Volumes/...' when saving, logging or loading a model.\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63b3ea8ed04045a78aea656ac9ab8a4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r 70%|███████   | 14/20 [03:19<01:25, 14.33s/trial, best loss: -1.0]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.000570 seconds\n\nINFO:hyperopt.tpe:TPE using 14/14 trials with best loss -1.000000\n\n2025/11/23 19:05:29 WARNING mlflow.utils.requirements_utils: Found pyspark version (4.0.0+databricks.connect.17.2.2) contains a local version label (+databricks.connect.17.2.2). MLflow logged a pip requirement for this package as 'pyspark==4.0.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n\n2025/11/23 19:05:31 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /local_disk0/user_tmp_data/spark-4fa5dc13-0dd2-4ab5-8c0f-4e/tmpzsmkyu02/model, flavor: spark). Fall back to return ['pyspark==4.0.0']. Set logging level to DEBUG to see the full traceback. \n\n2025/11/23 19:05:31 WARNING mlflow.models.model: Failed to validate serving input example {\n  \"inputs\": {\n    \"features_0\": 0.0,\n    \"features_1\": 0.0,\n    \"features_2\": 0.0,\n    \"features_3\": 0.0,\n    \"features_4\": 0.0,\n    \"features_5\": 0.495243504188556,\n    \"features_6\": 0.09683373562402385,\n    \"features_7\": 0.34374556297032516,\n    \"features_8\": 0.2866676132329973,\n    \"features_9\": 0.3448814425670879,\n    \"features_10\": 0.43944341899758627,\n    \"features_11\": 0.2902172369728809,\n    \"features_12\": 0.3989777083629135,\n    \"features_13\": 0.39542808462303,\n    \"features_14\": 0.20914383075394008,\n    \"features_15\": 0.2161010932841119\n  }\n}. Alternatively, you can avoid passing input example and pass model signature instead when logging the model. To ensure the input example is valid prior to serving, please try calling `mlflow.models.validate_serving_input` on the model uri and serving input example. A serving input example can be generated from model input example using `mlflow.models.convert_input_example_to_serving_input` function.\nGot error: UC volume path must be provided to save, log or load SparkML models in Databricks shared or serverless clusters. Specify environment variable 'MLFLOW_DFS_TMP' or 'dfs_tmpdir' argument that uses a UC volume path starting with '/Volumes/...' when saving, logging or loading a model.\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a95d82c0c03427ea2b29d6f4b52175f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r 75%|███████▌  | 15/20 [03:33<01:11, 14.29s/trial, best loss: -1.0]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.000567 seconds\n\nINFO:hyperopt.tpe:TPE using 15/15 trials with best loss -1.000000\n\n2025/11/23 19:05:43 WARNING mlflow.utils.requirements_utils: Found pyspark version (4.0.0+databricks.connect.17.2.2) contains a local version label (+databricks.connect.17.2.2). MLflow logged a pip requirement for this package as 'pyspark==4.0.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n\n2025/11/23 19:05:45 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /local_disk0/user_tmp_data/spark-4fa5dc13-0dd2-4ab5-8c0f-4e/tmp59yvuhg6/model, flavor: spark). Fall back to return ['pyspark==4.0.0']. Set logging level to DEBUG to see the full traceback. \n\n2025/11/23 19:05:46 WARNING mlflow.models.model: Failed to validate serving input example {\n  \"inputs\": {\n    \"features_0\": 0.0,\n    \"features_1\": 0.0,\n    \"features_2\": 0.0,\n    \"features_3\": 0.0,\n    \"features_4\": 0.0,\n    \"features_5\": 0.495243504188556,\n    \"features_6\": 0.09683373562402385,\n    \"features_7\": 0.34374556297032516,\n    \"features_8\": 0.2866676132329973,\n    \"features_9\": 0.3448814425670879,\n    \"features_10\": 0.43944341899758627,\n    \"features_11\": 0.2902172369728809,\n    \"features_12\": 0.3989777083629135,\n    \"features_13\": 0.39542808462303,\n    \"features_14\": 0.20914383075394008,\n    \"features_15\": 0.2161010932841119\n  }\n}. Alternatively, you can avoid passing input example and pass model signature instead when logging the model. To ensure the input example is valid prior to serving, please try calling `mlflow.models.validate_serving_input` on the model uri and serving input example. A serving input example can be generated from model input example using `mlflow.models.convert_input_example_to_serving_input` function.\nGot error: UC volume path must be provided to save, log or load SparkML models in Databricks shared or serverless clusters. Specify environment variable 'MLFLOW_DFS_TMP' or 'dfs_tmpdir' argument that uses a UC volume path starting with '/Volumes/...' when saving, logging or loading a model.\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b614c5891512439cb893cb1cc895dafd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r 80%|████████  | 16/20 [03:48<00:56, 14.22s/trial, best loss: -1.0]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.000590 seconds\n\nINFO:hyperopt.tpe:TPE using 16/16 trials with best loss -1.000000\n\n2025/11/23 19:05:58 WARNING mlflow.utils.requirements_utils: Found pyspark version (4.0.0+databricks.connect.17.2.2) contains a local version label (+databricks.connect.17.2.2). MLflow logged a pip requirement for this package as 'pyspark==4.0.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n\n2025/11/23 19:06:00 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /local_disk0/user_tmp_data/spark-4fa5dc13-0dd2-4ab5-8c0f-4e/tmp0uvfrw2o/model, flavor: spark). Fall back to return ['pyspark==4.0.0']. Set logging level to DEBUG to see the full traceback. \n\n2025/11/23 19:06:00 WARNING mlflow.models.model: Failed to validate serving input example {\n  \"inputs\": {\n    \"features_0\": 0.0,\n    \"features_1\": 0.0,\n    \"features_2\": 0.0,\n    \"features_3\": 0.0,\n    \"features_4\": 0.0,\n    \"features_5\": 0.495243504188556,\n    \"features_6\": 0.09683373562402385,\n    \"features_7\": 0.34374556297032516,\n    \"features_8\": 0.2866676132329973,\n    \"features_9\": 0.3448814425670879,\n    \"features_10\": 0.43944341899758627,\n    \"features_11\": 0.2902172369728809,\n    \"features_12\": 0.3989777083629135,\n    \"features_13\": 0.39542808462303,\n    \"features_14\": 0.20914383075394008,\n    \"features_15\": 0.2161010932841119\n  }\n}. Alternatively, you can avoid passing input example and pass model signature instead when logging the model. To ensure the input example is valid prior to serving, please try calling `mlflow.models.validate_serving_input` on the model uri and serving input example. A serving input example can be generated from model input example using `mlflow.models.convert_input_example_to_serving_input` function.\nGot error: UC volume path must be provided to save, log or load SparkML models in Databricks shared or serverless clusters. Specify environment variable 'MLFLOW_DFS_TMP' or 'dfs_tmpdir' argument that uses a UC volume path starting with '/Volumes/...' when saving, logging or loading a model.\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35738c26d1e4478b969264ea6088a2f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r 85%|████████▌ | 17/20 [04:02<00:42, 14.33s/trial, best loss: -1.0]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.000577 seconds\n\nINFO:hyperopt.tpe:TPE using 17/17 trials with best loss -1.000000\n\n2025/11/23 19:06:11 WARNING mlflow.utils.requirements_utils: Found pyspark version (4.0.0+databricks.connect.17.2.2) contains a local version label (+databricks.connect.17.2.2). MLflow logged a pip requirement for this package as 'pyspark==4.0.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n\n2025/11/23 19:06:14 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /local_disk0/user_tmp_data/spark-4fa5dc13-0dd2-4ab5-8c0f-4e/tmpn6upq0_e/model, flavor: spark). Fall back to return ['pyspark==4.0.0']. Set logging level to DEBUG to see the full traceback. \n\n2025/11/23 19:06:14 WARNING mlflow.models.model: Failed to validate serving input example {\n  \"inputs\": {\n    \"features_0\": 0.0,\n    \"features_1\": 0.0,\n    \"features_2\": 0.0,\n    \"features_3\": 0.0,\n    \"features_4\": 0.0,\n    \"features_5\": 0.495243504188556,\n    \"features_6\": 0.09683373562402385,\n    \"features_7\": 0.34374556297032516,\n    \"features_8\": 0.2866676132329973,\n    \"features_9\": 0.3448814425670879,\n    \"features_10\": 0.43944341899758627,\n    \"features_11\": 0.2902172369728809,\n    \"features_12\": 0.3989777083629135,\n    \"features_13\": 0.39542808462303,\n    \"features_14\": 0.20914383075394008,\n    \"features_15\": 0.2161010932841119\n  }\n}. Alternatively, you can avoid passing input example and pass model signature instead when logging the model. To ensure the input example is valid prior to serving, please try calling `mlflow.models.validate_serving_input` on the model uri and serving input example. A serving input example can be generated from model input example using `mlflow.models.convert_input_example_to_serving_input` function.\nGot error: UC volume path must be provided to save, log or load SparkML models in Databricks shared or serverless clusters. Specify environment variable 'MLFLOW_DFS_TMP' or 'dfs_tmpdir' argument that uses a UC volume path starting with '/Volumes/...' when saving, logging or loading a model.\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dd390dfab054ecfab307da877343551",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r 90%|█████████ | 18/20 [04:16<00:28, 14.13s/trial, best loss: -1.0]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.000593 seconds\n\nINFO:hyperopt.tpe:TPE using 18/18 trials with best loss -1.000000\n\n2025/11/23 19:06:25 WARNING mlflow.utils.requirements_utils: Found pyspark version (4.0.0+databricks.connect.17.2.2) contains a local version label (+databricks.connect.17.2.2). MLflow logged a pip requirement for this package as 'pyspark==4.0.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n\n2025/11/23 19:06:28 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /local_disk0/user_tmp_data/spark-4fa5dc13-0dd2-4ab5-8c0f-4e/tmpwbxj9jy6/model, flavor: spark). Fall back to return ['pyspark==4.0.0']. Set logging level to DEBUG to see the full traceback. \n\n2025/11/23 19:06:28 WARNING mlflow.models.model: Failed to validate serving input example {\n  \"inputs\": {\n    \"features_0\": 0.0,\n    \"features_1\": 0.0,\n    \"features_2\": 0.0,\n    \"features_3\": 0.0,\n    \"features_4\": 0.0,\n    \"features_5\": 0.495243504188556,\n    \"features_6\": 0.09683373562402385,\n    \"features_7\": 0.34374556297032516,\n    \"features_8\": 0.2866676132329973,\n    \"features_9\": 0.3448814425670879,\n    \"features_10\": 0.43944341899758627,\n    \"features_11\": 0.2902172369728809,\n    \"features_12\": 0.3989777083629135,\n    \"features_13\": 0.39542808462303,\n    \"features_14\": 0.20914383075394008,\n    \"features_15\": 0.2161010932841119\n  }\n}. Alternatively, you can avoid passing input example and pass model signature instead when logging the model. To ensure the input example is valid prior to serving, please try calling `mlflow.models.validate_serving_input` on the model uri and serving input example. A serving input example can be generated from model input example using `mlflow.models.convert_input_example_to_serving_input` function.\nGot error: UC volume path must be provided to save, log or load SparkML models in Databricks shared or serverless clusters. Specify environment variable 'MLFLOW_DFS_TMP' or 'dfs_tmpdir' argument that uses a UC volume path starting with '/Volumes/...' when saving, logging or loading a model.\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e59001f21e948b6896171468a0f5f6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r 95%|█████████▌| 19/20 [04:30<00:14, 14.00s/trial, best loss: -1.0]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.000590 seconds\n\nINFO:hyperopt.tpe:TPE using 19/19 trials with best loss -1.000000\n\n2025/11/23 19:06:39 WARNING mlflow.utils.requirements_utils: Found pyspark version (4.0.0+databricks.connect.17.2.2) contains a local version label (+databricks.connect.17.2.2). MLflow logged a pip requirement for this package as 'pyspark==4.0.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n\n2025/11/23 19:06:41 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /local_disk0/user_tmp_data/spark-4fa5dc13-0dd2-4ab5-8c0f-4e/tmpfv33mf18/model, flavor: spark). Fall back to return ['pyspark==4.0.0']. Set logging level to DEBUG to see the full traceback. \n\n2025/11/23 19:06:41 WARNING mlflow.models.model: Failed to validate serving input example {\n  \"inputs\": {\n    \"features_0\": 0.0,\n    \"features_1\": 0.0,\n    \"features_2\": 0.0,\n    \"features_3\": 0.0,\n    \"features_4\": 0.0,\n    \"features_5\": 0.495243504188556,\n    \"features_6\": 0.09683373562402385,\n    \"features_7\": 0.34374556297032516,\n    \"features_8\": 0.2866676132329973,\n    \"features_9\": 0.3448814425670879,\n    \"features_10\": 0.43944341899758627,\n    \"features_11\": 0.2902172369728809,\n    \"features_12\": 0.3989777083629135,\n    \"features_13\": 0.39542808462303,\n    \"features_14\": 0.20914383075394008,\n    \"features_15\": 0.2161010932841119\n  }\n}. Alternatively, you can avoid passing input example and pass model signature instead when logging the model. To ensure the input example is valid prior to serving, please try calling `mlflow.models.validate_serving_input` on the model uri and serving input example. A serving input example can be generated from model input example using `mlflow.models.convert_input_example_to_serving_input` function.\nGot error: UC volume path must be provided to save, log or load SparkML models in Databricks shared or serverless clusters. Specify environment variable 'MLFLOW_DFS_TMP' or 'dfs_tmpdir' argument that uses a UC volume path starting with '/Volumes/...' when saving, logging or loading a model.\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c06175168854d22a52616972d234ba3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r100%|██████████| 20/20 [04:43<00:00, 13.86s/trial, best loss: -1.0]\r100%|██████████| 20/20 [04:43<00:00, 14.18s/trial, best loss: -1.0]\n"
     ]
    }
   ],
   "source": [
    "#Asignación de variable de entorno apuntar a volumen para guardar modelo\n",
    "VOLUME_TMP = \"/Volumes/corona/gold/models/ml_tmp\"\n",
    "\n",
    "#Función para pasar el vector a un array\n",
    "def vector_to_columns(df, vector_col=\"features\"):\n",
    "    #convertir vector -> array<double>\n",
    "    df = df.withColumn(f\"{vector_col}_arr\", vector_to_array(vector_col))\n",
    "\n",
    "    #obtener tamaño\n",
    "    first = df.select(f\"{vector_col}_arr\").head()\n",
    "    size = len(first[f\"{vector_col}_arr\"])\n",
    "\n",
    "    #crear columnas primitivas\n",
    "    for i in range(size):\n",
    "        df = df.withColumn(f\"{vector_col}_{i}\", df[f\"{vector_col}_arr\"][i])\n",
    "\n",
    "    return df, size\n",
    "\n",
    "\n",
    "def build_signature_and_example(preds):\n",
    "    \"\"\"\n",
    "    Convierte features vector en array para el signature. esto es netamente para el registro, ya que evita errores con el registro de vector de predicciones\n",
    "    \"\"\"\n",
    "    \n",
    "    preds_cols, vec_size = vector_to_columns(preds, \"features\")\n",
    "\n",
    "    input_columns = [f\"features_{i}\" for i in range(vec_size)]\n",
    "\n",
    "    input_example = preds_cols.select(input_columns).limit(1).toPandas().to_dict(orient=\"records\")[0]\n",
    "\n",
    "    input_schema = Schema([ColSpec(\"double\", col) for col in input_columns])\n",
    "    output_schema = Schema([ColSpec(\"double\")])\n",
    "\n",
    "    signature = ModelSignature(inputs=input_schema, outputs=output_schema)\n",
    "\n",
    "    return signature, input_example\n",
    "\n",
    "#Función objetivo de regresión logistica, \n",
    "def objectivo_lr(params):\n",
    "\n",
    "    with mlflow.start_run(run_name=\"LR_Hyperopt\"):\n",
    "\n",
    "        lr = LogisticRegression(\n",
    "            featuresCol=\"features\",\n",
    "            labelCol=\"Churn\",\n",
    "            regParam=params[\"regParam\"],\n",
    "            elasticNetParam=params[\"elasticNetParam\"]\n",
    "        )\n",
    "\n",
    "        model = lr.fit(train_df)\n",
    "        preds = model.transform(test_df)\n",
    "\n",
    "        auc, acc, f1 = compute_metrics(preds)\n",
    "\n",
    "        signature, input_example = build_signature_and_example(preds)\n",
    "\n",
    "        mlflow.log_params(params)\n",
    "        mlflow.log_metric(\"AUC\", auc)\n",
    "        mlflow.log_metric(\"Accuracy\", acc)\n",
    "        mlflow.log_metric(\"F1\", f1)\n",
    "\n",
    "        mlflow.spark.log_model(\n",
    "            model,\n",
    "            artifact_path=\"LR_Model\",\n",
    "            signature=signature,\n",
    "            input_example=input_example,\n",
    "            dfs_tmpdir=VOLUME_TMP\n",
    "        )\n",
    "\n",
    "    return {\"loss\": -auc, \"status\": STATUS_OK}\n",
    "\n",
    "#Hiperparametros opcionales\n",
    "space_lr = {\n",
    "    \"regParam\": hp.choice(\"regParam\", [0.0, 0.001, 0.01, 0.1, 0.3]),\n",
    "    \"elasticNetParam\": hp.uniform(\"elasticNetParam\", 0.0, 1.0)\n",
    "}\n",
    "\n",
    "trials_lr = Trials()\n",
    "\n",
    "#Aplicación y registro de entrenamiento de modelo con Hyperopt\n",
    "best_lr = fmin(\n",
    "    fn=objectivo_lr,\n",
    "    space=space_lr,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=20,\n",
    "    trials=trials_lr\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc1a1466-000e-4921-a5b3-c1cc537fc0da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p0\n\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------> Mejores hiperparámetros LR son: {'elasticNetParam': np.float64(0.36828278457007546), 'regParam': np.int64(0)}\n"
     ]
    }
   ],
   "source": [
    "print(\"------------> Mejores hiperparámetros LR son:\", best_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "770af470-6817-4901-84ea-79ad8439d1b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Selección y Registro del Modelo - Regresión Logistica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c3ec725-70ec-4a2a-aafe-efdb50e5c785",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully registered model 'workspace.default.churn_lr_model'.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5f0cc23c2bf4e47b4a1879c0934cb7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d58eb2613aa64bbea65203ec60601279",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Closing down clientserver connection\n\nINFO:py4j.clientserver:Closing down clientserver connection\n\nINFO:py4j.clientserver:Closing down clientserver connection\n\nINFO:py4j.clientserver:Closing down clientserver connection\n\nINFO:py4j.clientserver:Closing down clientserver connection\n\nINFO:py4j.clientserver:Closing down clientserver connection\n\nINFO:py4j.clientserver:Closing down clientserver connection\n\nINFO:py4j.clientserver:Closing down clientserver connection\n\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Logistic Regression best model registered: <ModelVersion: aliases=[], creation_timestamp=1763925166681, current_stage=None, description='', last_updated_timestamp=1763925169855, name='workspace.default.churn_lr_model', run_id='d6e63ea57d8e48eba395bccf23a3fb78', run_link=None, source='dbfs:/databricks/mlflow-tracking/2671492619073786/d6e63ea57d8e48eba395bccf23a3fb78/artifacts/LR_Model', status='READY', status_message='', tags={}, user_id='josestunt95@gmail.com', version='1'>\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '1' of model 'workspace.default.churn_lr_model'.\n"
     ]
    }
   ],
   "source": [
    "#Consulta MLflow para seleccionar la mejor LR por AUC como base\n",
    "best_lr_run_info = mlflow.search_runs(\n",
    "filter_string=\"tags.mlflow.runName = 'LR_Hyperopt'\",\n",
    "order_by=[\"metrics.AUC DESC\"],\n",
    "max_results=1\n",
    ").iloc[0]\n",
    "\n",
    "\n",
    "best_lr_run_id = best_lr_run_info.run_id\n",
    "lr_model_uri = f\"runs:/{best_lr_run_id}/LR_Model\"\n",
    "\n",
    "\n",
    "lr_registered = mlflow.register_model(\n",
    "model_uri=lr_model_uri,\n",
    "name=\"Churn_LR_Model\"\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Mejor modelo de regresión logistica registrado:\", lr_registered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15484d65-da54-41cd-88de-7f6a9be4adbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff012241-3d1d-472f-b4c5-0051f8136350",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p0\n\n"
     ]
    }
   ],
   "source": [
    "#Función objetivo de aplicación de Random Forest\n",
    "def objectivo_rf(params):\n",
    "    with mlflow.start_run(run_name=\"RF_Hyperopt\"):\n",
    "        rf = RandomForestClassifier(\n",
    "            featuresCol=\"features\",\n",
    "            labelCol=\"Churn\",\n",
    "            numTrees=int(params[\"numTrees\"]),\n",
    "            maxDepth=int(params[\"maxDepth\"])\n",
    "        )\n",
    "\n",
    "        model = rf.fit(train_df)\n",
    "        preds = model.transform(test_df)\n",
    "\n",
    "        auc, acc, f1 = compute_metrics(preds)\n",
    "\n",
    "        signature, input_example = build_signature_and_example(preds)\n",
    "\n",
    "        mlflow.log_params(params)\n",
    "        mlflow.log_metric(\"AUC\", auc)\n",
    "        mlflow.log_metric(\"Accuracy\", acc)\n",
    "        mlflow.log_metric(\"F1\", f1)\n",
    "\n",
    "        mlflow.spark.log_model(\n",
    "            model,\n",
    "            artifact_path=\"RF_Model\",\n",
    "            signature=signature,\n",
    "            input_example=input_example,\n",
    "            dfs_tmpdir=VOLUME_TMP\n",
    "        )\n",
    "\n",
    "    return {\"loss\": -auc, \"status\": STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3eccf50c-a3fb-470f-84a0-84d3534bcb35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Opción de hiperparametros a probar en el modelo\n",
    "space_rf = {\n",
    "\"numTrees\": hp.choice(\"numTrees\", [10, 20, 30, 40, 50]),\n",
    "\"maxDepth\": hp.choice(\"maxDepth\", [3, 5, 7, 10])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55b3fec1-e5e7-4cb1-9f91-1d0112ea3494",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 0/20 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.001139 seconds\n\nINFO:hyperopt.tpe:TPE using 0 trials\n\n2025/11/23 19:26:49 WARNING mlflow.utils.requirements_utils: Found pyspark version (4.0.0+databricks.connect.17.2.2) contains a local version label (+databricks.connect.17.2.2). MLflow logged a pip requirement for this package as 'pyspark==4.0.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n\n2025/11/23 19:26:52 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /local_disk0/user_tmp_data/spark-4fa5dc13-0dd2-4ab5-8c0f-4e/tmplawhr3ix/model, flavor: spark). Fall back to return ['pyspark==4.0.0']. Set logging level to DEBUG to see the full traceback. \n\n2025/11/23 19:26:52 WARNING mlflow.models.model: Failed to validate serving input example {\n  \"inputs\": {\n    \"features_0\": 0.0,\n    \"features_1\": 0.0,\n    \"features_2\": 0.0,\n    \"features_3\": 0.0,\n    \"features_4\": 0.0,\n    \"features_5\": 0.495243504188556,\n    \"features_6\": 0.09683373562402385,\n    \"features_7\": 0.34374556297032516,\n    \"features_8\": 0.2866676132329973,\n    \"features_9\": 0.3448814425670879,\n    \"features_10\": 0.43944341899758627,\n    \"features_11\": 0.2902172369728809,\n    \"features_12\": 0.3989777083629135,\n    \"features_13\": 0.39542808462303,\n    \"features_14\": 0.20914383075394008,\n    \"features_15\": 0.2161010932841119\n  }\n}. Alternatively, you can avoid passing input example and pass model signature instead when logging the model. To ensure the input example is valid prior to serving, please try calling `mlflow.models.validate_serving_input` on the model uri and serving input example. A serving input example can be generated from model input example using `mlflow.models.convert_input_example_to_serving_input` function.\nGot error: UC volume path must be provided to save, log or load SparkML models in Databricks shared or serverless clusters. Specify environment variable 'MLFLOW_DFS_TMP' or 'dfs_tmpdir' argument that uses a UC volume path starting with '/Volumes/...' when saving, logging or loading a model.\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "830b9837697041c594cb1b0edab7bf8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r  5%|▌         | 1/20 [00:17<05:30, 17.40s/trial, best loss: -1.0]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.000671 seconds\n\nINFO:hyperopt.tpe:TPE using 1/1 trials with best loss -1.000000\n\n2025/11/23 19:27:07 WARNING mlflow.utils.requirements_utils: Found pyspark version (4.0.0+databricks.connect.17.2.2) contains a local version label (+databricks.connect.17.2.2). MLflow logged a pip requirement for this package as 'pyspark==4.0.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n\n2025/11/23 19:27:09 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /local_disk0/user_tmp_data/spark-4fa5dc13-0dd2-4ab5-8c0f-4e/tmpdoh8io_t/model, flavor: spark). Fall back to return ['pyspark==4.0.0']. Set logging level to DEBUG to see the full traceback. \n\n2025/11/23 19:27:09 WARNING mlflow.models.model: Failed to validate serving input example {\n  \"inputs\": {\n    \"features_0\": 0.0,\n    \"features_1\": 0.0,\n    \"features_2\": 0.0,\n    \"features_3\": 0.0,\n    \"features_4\": 0.0,\n    \"features_5\": 0.495243504188556,\n    \"features_6\": 0.09683373562402385,\n    \"features_7\": 0.34374556297032516,\n    \"features_8\": 0.2866676132329973,\n    \"features_9\": 0.3448814425670879,\n    \"features_10\": 0.43944341899758627,\n    \"features_11\": 0.2902172369728809,\n    \"features_12\": 0.3989777083629135,\n    \"features_13\": 0.39542808462303,\n    \"features_14\": 0.20914383075394008,\n    \"features_15\": 0.2161010932841119\n  }\n}. Alternatively, you can avoid passing input example and pass model signature instead when logging the model. To ensure the input example is valid prior to serving, please try calling `mlflow.models.validate_serving_input` on the model uri and serving input example. A serving input example can be generated from model input example using `mlflow.models.convert_input_example_to_serving_input` function.\nGot error: UC volume path must be provided to save, log or load SparkML models in Databricks shared or serverless clusters. Specify environment variable 'MLFLOW_DFS_TMP' or 'dfs_tmpdir' argument that uses a UC volume path starting with '/Volumes/...' when saving, logging or loading a model.\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bafdedc0201407c8f3295dc07b40a7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r 10%|█         | 2/20 [00:34<05:06, 17.05s/trial, best loss: -1.0]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.000649 seconds\n\nINFO:hyperopt.tpe:TPE using 2/2 trials with best loss -1.000000\n\n2025/11/23 19:27:23 WARNING mlflow.utils.requirements_utils: Found pyspark version (4.0.0+databricks.connect.17.2.2) contains a local version label (+databricks.connect.17.2.2). MLflow logged a pip requirement for this package as 'pyspark==4.0.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n\n2025/11/23 19:27:25 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /local_disk0/user_tmp_data/spark-4fa5dc13-0dd2-4ab5-8c0f-4e/tmp2lnmh49j/model, flavor: spark). Fall back to return ['pyspark==4.0.0']. Set logging level to DEBUG to see the full traceback. \n\n2025/11/23 19:27:25 WARNING mlflow.models.model: Failed to validate serving input example {\n  \"inputs\": {\n    \"features_0\": 0.0,\n    \"features_1\": 0.0,\n    \"features_2\": 0.0,\n    \"features_3\": 0.0,\n    \"features_4\": 0.0,\n    \"features_5\": 0.495243504188556,\n    \"features_6\": 0.09683373562402385,\n    \"features_7\": 0.34374556297032516,\n    \"features_8\": 0.2866676132329973,\n    \"features_9\": 0.3448814425670879,\n    \"features_10\": 0.43944341899758627,\n    \"features_11\": 0.2902172369728809,\n    \"features_12\": 0.3989777083629135,\n    \"features_13\": 0.39542808462303,\n    \"features_14\": 0.20914383075394008,\n    \"features_15\": 0.2161010932841119\n  }\n}. Alternatively, you can avoid passing input example and pass model signature instead when logging the model. To ensure the input example is valid prior to serving, please try calling `mlflow.models.validate_serving_input` on the model uri and serving input example. A serving input example can be generated from model input example using `mlflow.models.convert_input_example_to_serving_input` function.\nGot error: UC volume path must be provided to save, log or load SparkML models in Databricks shared or serverless clusters. Specify environment variable 'MLFLOW_DFS_TMP' or 'dfs_tmpdir' argument that uses a UC volume path starting with '/Volumes/...' when saving, logging or loading a model.\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21708e6f34c94316b09c8f815e47e49f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r 15%|█▌        | 3/20 [00:52<04:56, 17.47s/trial, best loss: -1.0]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.000660 seconds\n\nINFO:hyperopt.tpe:TPE using 3/3 trials with best loss -1.000000\n\n2025/11/23 19:27:40 WARNING mlflow.utils.requirements_utils: Found pyspark version (4.0.0+databricks.connect.17.2.2) contains a local version label (+databricks.connect.17.2.2). MLflow logged a pip requirement for this package as 'pyspark==4.0.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n\n2025/11/23 19:27:42 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /local_disk0/user_tmp_data/spark-4fa5dc13-0dd2-4ab5-8c0f-4e/tmp1p0bdneg/model, flavor: spark). Fall back to return ['pyspark==4.0.0']. Set logging level to DEBUG to see the full traceback. \n\n2025/11/23 19:27:42 WARNING mlflow.models.model: Failed to validate serving input example {\n  \"inputs\": {\n    \"features_0\": 0.0,\n    \"features_1\": 0.0,\n    \"features_2\": 0.0,\n    \"features_3\": 0.0,\n    \"features_4\": 0.0,\n    \"features_5\": 0.495243504188556,\n    \"features_6\": 0.09683373562402385,\n    \"features_7\": 0.34374556297032516,\n    \"features_8\": 0.2866676132329973,\n    \"features_9\": 0.3448814425670879,\n    \"features_10\": 0.43944341899758627,\n    \"features_11\": 0.2902172369728809,\n    \"features_12\": 0.3989777083629135,\n    \"features_13\": 0.39542808462303,\n    \"features_14\": 0.20914383075394008,\n    \"features_15\": 0.2161010932841119\n  }\n}. Alternatively, you can avoid passing input example and pass model signature instead when logging the model. To ensure the input example is valid prior to serving, please try calling `mlflow.models.validate_serving_input` on the model uri and serving input example. A serving input example can be generated from model input example using `mlflow.models.convert_input_example_to_serving_input` function.\nGot error: UC volume path must be provided to save, log or load SparkML models in Databricks shared or serverless clusters. Specify environment variable 'MLFLOW_DFS_TMP' or 'dfs_tmpdir' argument that uses a UC volume path starting with '/Volumes/...' when saving, logging or loading a model.\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7b69c9006474718be4dd8d8b47e2006",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r 20%|██        | 4/20 [01:13<05:01, 18.82s/trial, best loss: -1.0]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.000703 seconds\n\nINFO:hyperopt.tpe:TPE using 4/4 trials with best loss -1.000000\n\n2025/11/23 19:28:01 WARNING mlflow.utils.requirements_utils: Found pyspark version (4.0.0+databricks.connect.17.2.2) contains a local version label (+databricks.connect.17.2.2). MLflow logged a pip requirement for this package as 'pyspark==4.0.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n\n2025/11/23 19:28:04 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /local_disk0/user_tmp_data/spark-4fa5dc13-0dd2-4ab5-8c0f-4e/tmp__il3qu_/model, flavor: spark). Fall back to return ['pyspark==4.0.0']. Set logging level to DEBUG to see the full traceback. \n\n2025/11/23 19:28:04 WARNING mlflow.models.model: Failed to validate serving input example {\n  \"inputs\": {\n    \"features_0\": 0.0,\n    \"features_1\": 0.0,\n    \"features_2\": 0.0,\n    \"features_3\": 0.0,\n    \"features_4\": 0.0,\n    \"features_5\": 0.495243504188556,\n    \"features_6\": 0.09683373562402385,\n    \"features_7\": 0.34374556297032516,\n    \"features_8\": 0.2866676132329973,\n    \"features_9\": 0.3448814425670879,\n    \"features_10\": 0.43944341899758627,\n    \"features_11\": 0.2902172369728809,\n    \"features_12\": 0.3989777083629135,\n    \"features_13\": 0.39542808462303,\n    \"features_14\": 0.20914383075394008,\n    \"features_15\": 0.2161010932841119\n  }\n}. Alternatively, you can avoid passing input example and pass model signature instead when logging the model. To ensure the input example is valid prior to serving, please try calling `mlflow.models.validate_serving_input` on the model uri and serving input example. A serving input example can be generated from model input example using `mlflow.models.convert_input_example_to_serving_input` function.\nGot error: UC volume path must be provided to save, log or load SparkML models in Databricks shared or serverless clusters. Specify environment variable 'MLFLOW_DFS_TMP' or 'dfs_tmpdir' argument that uses a UC volume path starting with '/Volumes/...' when saving, logging or loading a model.\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fabad6a1b97b4e48b82eb82afc6d0481",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r 25%|██▌       | 5/20 [01:31<04:40, 18.73s/trial, best loss: -1.0]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.000660 seconds\n\nINFO:hyperopt.tpe:TPE using 5/5 trials with best loss -1.000000\n\n2025/11/23 19:28:19 WARNING mlflow.utils.requirements_utils: Found pyspark version (4.0.0+databricks.connect.17.2.2) contains a local version label (+databricks.connect.17.2.2). MLflow logged a pip requirement for this package as 'pyspark==4.0.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n\n2025/11/23 19:28:22 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /local_disk0/user_tmp_data/spark-4fa5dc13-0dd2-4ab5-8c0f-4e/tmpt0sa_df9/model, flavor: spark). Fall back to return ['pyspark==4.0.0']. Set logging level to DEBUG to see the full traceback. \n\n2025/11/23 19:28:22 WARNING mlflow.models.model: Failed to validate serving input example {\n  \"inputs\": {\n    \"features_0\": 0.0,\n    \"features_1\": 0.0,\n    \"features_2\": 0.0,\n    \"features_3\": 0.0,\n    \"features_4\": 0.0,\n    \"features_5\": 0.495243504188556,\n    \"features_6\": 0.09683373562402385,\n    \"features_7\": 0.34374556297032516,\n    \"features_8\": 0.2866676132329973,\n    \"features_9\": 0.3448814425670879,\n    \"features_10\": 0.43944341899758627,\n    \"features_11\": 0.2902172369728809,\n    \"features_12\": 0.3989777083629135,\n    \"features_13\": 0.39542808462303,\n    \"features_14\": 0.20914383075394008,\n    \"features_15\": 0.2161010932841119\n  }\n}. Alternatively, you can avoid passing input example and pass model signature instead when logging the model. To ensure the input example is valid prior to serving, please try calling `mlflow.models.validate_serving_input` on the model uri and serving input example. A serving input example can be generated from model input example using `mlflow.models.convert_input_example_to_serving_input` function.\nGot error: UC volume path must be provided to save, log or load SparkML models in Databricks shared or serverless clusters. Specify environment variable 'MLFLOW_DFS_TMP' or 'dfs_tmpdir' argument that uses a UC volume path starting with '/Volumes/...' when saving, logging or loading a model.\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29e6e453005e416eb26cfda4b47660d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r 30%|███       | 6/20 [01:48<04:13, 18.10s/trial, best loss: -1.0]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.000649 seconds\n\nINFO:hyperopt.tpe:TPE using 6/6 trials with best loss -1.000000\n\n2025/11/23 19:28:37 WARNING mlflow.utils.requirements_utils: Found pyspark version (4.0.0+databricks.connect.17.2.2) contains a local version label (+databricks.connect.17.2.2). MLflow logged a pip requirement for this package as 'pyspark==4.0.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n\n2025/11/23 19:28:39 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /local_disk0/user_tmp_data/spark-4fa5dc13-0dd2-4ab5-8c0f-4e/tmpt884k7g0/model, flavor: spark). Fall back to return ['pyspark==4.0.0']. Set logging level to DEBUG to see the full traceback. \n\n2025/11/23 19:28:39 WARNING mlflow.models.model: Failed to validate serving input example {\n  \"inputs\": {\n    \"features_0\": 0.0,\n    \"features_1\": 0.0,\n    \"features_2\": 0.0,\n    \"features_3\": 0.0,\n    \"features_4\": 0.0,\n    \"features_5\": 0.495243504188556,\n    \"features_6\": 0.09683373562402385,\n    \"features_7\": 0.34374556297032516,\n    \"features_8\": 0.2866676132329973,\n    \"features_9\": 0.3448814425670879,\n    \"features_10\": 0.43944341899758627,\n    \"features_11\": 0.2902172369728809,\n    \"features_12\": 0.3989777083629135,\n    \"features_13\": 0.39542808462303,\n    \"features_14\": 0.20914383075394008,\n    \"features_15\": 0.2161010932841119\n  }\n}. Alternatively, you can avoid passing input example and pass model signature instead when logging the model. To ensure the input example is valid prior to serving, please try calling `mlflow.models.validate_serving_input` on the model uri and serving input example. A serving input example can be generated from model input example using `mlflow.models.convert_input_example_to_serving_input` function.\nGot error: UC volume path must be provided to save, log or load SparkML models in Databricks shared or serverless clusters. Specify environment variable 'MLFLOW_DFS_TMP' or 'dfs_tmpdir' argument that uses a UC volume path starting with '/Volumes/...' when saving, logging or loading a model.\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed6a1d87c99b410e883626f4d1b48dca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r 35%|███▌      | 7/20 [02:05<03:50, 17.76s/trial, best loss: -1.0]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.000663 seconds\n\nINFO:hyperopt.tpe:TPE using 7/7 trials with best loss -1.000000\n\n2025/11/23 19:28:54 WARNING mlflow.utils.requirements_utils: Found pyspark version (4.0.0+databricks.connect.17.2.2) contains a local version label (+databricks.connect.17.2.2). MLflow logged a pip requirement for this package as 'pyspark==4.0.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n\n2025/11/23 19:28:56 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /local_disk0/user_tmp_data/spark-4fa5dc13-0dd2-4ab5-8c0f-4e/tmpwxtd38_3/model, flavor: spark). Fall back to return ['pyspark==4.0.0']. Set logging level to DEBUG to see the full traceback. \n\n2025/11/23 19:28:56 WARNING mlflow.models.model: Failed to validate serving input example {\n  \"inputs\": {\n    \"features_0\": 0.0,\n    \"features_1\": 0.0,\n    \"features_2\": 0.0,\n    \"features_3\": 0.0,\n    \"features_4\": 0.0,\n    \"features_5\": 0.495243504188556,\n    \"features_6\": 0.09683373562402385,\n    \"features_7\": 0.34374556297032516,\n    \"features_8\": 0.2866676132329973,\n    \"features_9\": 0.3448814425670879,\n    \"features_10\": 0.43944341899758627,\n    \"features_11\": 0.2902172369728809,\n    \"features_12\": 0.3989777083629135,\n    \"features_13\": 0.39542808462303,\n    \"features_14\": 0.20914383075394008,\n    \"features_15\": 0.2161010932841119\n  }\n}. Alternatively, you can avoid passing input example and pass model signature instead when logging the model. To ensure the input example is valid prior to serving, please try calling `mlflow.models.validate_serving_input` on the model uri and serving input example. A serving input example can be generated from model input example using `mlflow.models.convert_input_example_to_serving_input` function.\nGot error: UC volume path must be provided to save, log or load SparkML models in Databricks shared or serverless clusters. Specify environment variable 'MLFLOW_DFS_TMP' or 'dfs_tmpdir' argument that uses a UC volume path starting with '/Volumes/...' when saving, logging or loading a model.\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edc5695e0a9647ba8c070fa275232234",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r 40%|████      | 8/20 [02:22<03:30, 17.57s/trial, best loss: -1.0]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.000639 seconds\n\nINFO:hyperopt.tpe:TPE using 8/8 trials with best loss -1.000000\n\n2025/11/23 19:29:10 WARNING mlflow.utils.requirements_utils: Found pyspark version (4.0.0+databricks.connect.17.2.2) contains a local version label (+databricks.connect.17.2.2). MLflow logged a pip requirement for this package as 'pyspark==4.0.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n\n2025/11/23 19:29:13 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /local_disk0/user_tmp_data/spark-4fa5dc13-0dd2-4ab5-8c0f-4e/tmp4tatrxw3/model, flavor: spark). Fall back to return ['pyspark==4.0.0']. Set logging level to DEBUG to see the full traceback. \n\n2025/11/23 19:29:13 WARNING mlflow.models.model: Failed to validate serving input example {\n  \"inputs\": {\n    \"features_0\": 0.0,\n    \"features_1\": 0.0,\n    \"features_2\": 0.0,\n    \"features_3\": 0.0,\n    \"features_4\": 0.0,\n    \"features_5\": 0.495243504188556,\n    \"features_6\": 0.09683373562402385,\n    \"features_7\": 0.34374556297032516,\n    \"features_8\": 0.2866676132329973,\n    \"features_9\": 0.3448814425670879,\n    \"features_10\": 0.43944341899758627,\n    \"features_11\": 0.2902172369728809,\n    \"features_12\": 0.3989777083629135,\n    \"features_13\": 0.39542808462303,\n    \"features_14\": 0.20914383075394008,\n    \"features_15\": 0.2161010932841119\n  }\n}. Alternatively, you can avoid passing input example and pass model signature instead when logging the model. To ensure the input example is valid prior to serving, please try calling `mlflow.models.validate_serving_input` on the model uri and serving input example. A serving input example can be generated from model input example using `mlflow.models.convert_input_example_to_serving_input` function.\nGot error: UC volume path must be provided to save, log or load SparkML models in Databricks shared or serverless clusters. Specify environment variable 'MLFLOW_DFS_TMP' or 'dfs_tmpdir' argument that uses a UC volume path starting with '/Volumes/...' when saving, logging or loading a model.\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1349d5a66b314a55b5041766435bd771",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r 45%|████▌     | 9/20 [02:37<03:03, 16.67s/trial, best loss: -1.0]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.000657 seconds\n\nINFO:hyperopt.tpe:TPE using 9/9 trials with best loss -1.000000\n\n2025/11/23 19:29:25 WARNING mlflow.utils.requirements_utils: Found pyspark version (4.0.0+databricks.connect.17.2.2) contains a local version label (+databricks.connect.17.2.2). MLflow logged a pip requirement for this package as 'pyspark==4.0.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n\n2025/11/23 19:29:27 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /local_disk0/user_tmp_data/spark-4fa5dc13-0dd2-4ab5-8c0f-4e/tmpe_mcp4vx/model, flavor: spark). Fall back to return ['pyspark==4.0.0']. Set logging level to DEBUG to see the full traceback. \n\n2025/11/23 19:29:27 WARNING mlflow.models.model: Failed to validate serving input example {\n  \"inputs\": {\n    \"features_0\": 0.0,\n    \"features_1\": 0.0,\n    \"features_2\": 0.0,\n    \"features_3\": 0.0,\n    \"features_4\": 0.0,\n    \"features_5\": 0.495243504188556,\n    \"features_6\": 0.09683373562402385,\n    \"features_7\": 0.34374556297032516,\n    \"features_8\": 0.2866676132329973,\n    \"features_9\": 0.3448814425670879,\n    \"features_10\": 0.43944341899758627,\n    \"features_11\": 0.2902172369728809,\n    \"features_12\": 0.3989777083629135,\n    \"features_13\": 0.39542808462303,\n    \"features_14\": 0.20914383075394008,\n    \"features_15\": 0.2161010932841119\n  }\n}. Alternatively, you can avoid passing input example and pass model signature instead when logging the model. To ensure the input example is valid prior to serving, please try calling `mlflow.models.validate_serving_input` on the model uri and serving input example. A serving input example can be generated from model input example using `mlflow.models.convert_input_example_to_serving_input` function.\nGot error: UC volume path must be provided to save, log or load SparkML models in Databricks shared or serverless clusters. Specify environment variable 'MLFLOW_DFS_TMP' or 'dfs_tmpdir' argument that uses a UC volume path starting with '/Volumes/...' when saving, logging or loading a model.\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "959cb64bb4754c6e93641798176586ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r 50%|█████     | 10/20 [02:52<02:42, 16.22s/trial, best loss: -1.0]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.000687 seconds\n\nINFO:hyperopt.tpe:TPE using 10/10 trials with best loss -1.000000\n\n2025/11/23 19:29:40 WARNING mlflow.utils.requirements_utils: Found pyspark version (4.0.0+databricks.connect.17.2.2) contains a local version label (+databricks.connect.17.2.2). MLflow logged a pip requirement for this package as 'pyspark==4.0.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n\n2025/11/23 19:29:42 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /local_disk0/user_tmp_data/spark-4fa5dc13-0dd2-4ab5-8c0f-4e/tmp_6nacfqd/model, flavor: spark). Fall back to return ['pyspark==4.0.0']. Set logging level to DEBUG to see the full traceback. \n\n2025/11/23 19:29:42 WARNING mlflow.models.model: Failed to validate serving input example {\n  \"inputs\": {\n    \"features_0\": 0.0,\n    \"features_1\": 0.0,\n    \"features_2\": 0.0,\n    \"features_3\": 0.0,\n    \"features_4\": 0.0,\n    \"features_5\": 0.495243504188556,\n    \"features_6\": 0.09683373562402385,\n    \"features_7\": 0.34374556297032516,\n    \"features_8\": 0.2866676132329973,\n    \"features_9\": 0.3448814425670879,\n    \"features_10\": 0.43944341899758627,\n    \"features_11\": 0.2902172369728809,\n    \"features_12\": 0.3989777083629135,\n    \"features_13\": 0.39542808462303,\n    \"features_14\": 0.20914383075394008,\n    \"features_15\": 0.2161010932841119\n  }\n}. Alternatively, you can avoid passing input example and pass model signature instead when logging the model. To ensure the input example is valid prior to serving, please try calling `mlflow.models.validate_serving_input` on the model uri and serving input example. A serving input example can be generated from model input example using `mlflow.models.convert_input_example_to_serving_input` function.\nGot error: UC volume path must be provided to save, log or load SparkML models in Databricks shared or serverless clusters. Specify environment variable 'MLFLOW_DFS_TMP' or 'dfs_tmpdir' argument that uses a UC volume path starting with '/Volumes/...' when saving, logging or loading a model.\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3b44ffe71674cfbaf7ea5b8615f5a26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r 55%|█████▌    | 11/20 [03:11<02:32, 16.89s/trial, best loss: -1.0]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.000648 seconds\n\nINFO:hyperopt.tpe:TPE using 11/11 trials with best loss -1.000000\n\n2025/11/23 19:29:58 WARNING mlflow.utils.requirements_utils: Found pyspark version (4.0.0+databricks.connect.17.2.2) contains a local version label (+databricks.connect.17.2.2). MLflow logged a pip requirement for this package as 'pyspark==4.0.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n\n2025/11/23 19:30:01 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /local_disk0/user_tmp_data/spark-4fa5dc13-0dd2-4ab5-8c0f-4e/tmpzjghk5mr/model, flavor: spark). Fall back to return ['pyspark==4.0.0']. Set logging level to DEBUG to see the full traceback. \n\n2025/11/23 19:30:01 WARNING mlflow.models.model: Failed to validate serving input example {\n  \"inputs\": {\n    \"features_0\": 0.0,\n    \"features_1\": 0.0,\n    \"features_2\": 0.0,\n    \"features_3\": 0.0,\n    \"features_4\": 0.0,\n    \"features_5\": 0.495243504188556,\n    \"features_6\": 0.09683373562402385,\n    \"features_7\": 0.34374556297032516,\n    \"features_8\": 0.2866676132329973,\n    \"features_9\": 0.3448814425670879,\n    \"features_10\": 0.43944341899758627,\n    \"features_11\": 0.2902172369728809,\n    \"features_12\": 0.3989777083629135,\n    \"features_13\": 0.39542808462303,\n    \"features_14\": 0.20914383075394008,\n    \"features_15\": 0.2161010932841119\n  }\n}. Alternatively, you can avoid passing input example and pass model signature instead when logging the model. To ensure the input example is valid prior to serving, please try calling `mlflow.models.validate_serving_input` on the model uri and serving input example. A serving input example can be generated from model input example using `mlflow.models.convert_input_example_to_serving_input` function.\nGot error: UC volume path must be provided to save, log or load SparkML models in Databricks shared or serverless clusters. Specify environment variable 'MLFLOW_DFS_TMP' or 'dfs_tmpdir' argument that uses a UC volume path starting with '/Volumes/...' when saving, logging or loading a model.\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "051365a7811244d8862d9da3c38bc8d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r 60%|██████    | 12/20 [03:29<02:18, 17.35s/trial, best loss: -1.0]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.000664 seconds\n\nINFO:hyperopt.tpe:TPE using 12/12 trials with best loss -1.000000\n\n2025/11/23 19:30:17 WARNING mlflow.utils.requirements_utils: Found pyspark version (4.0.0+databricks.connect.17.2.2) contains a local version label (+databricks.connect.17.2.2). MLflow logged a pip requirement for this package as 'pyspark==4.0.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n\n2025/11/23 19:30:19 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /local_disk0/user_tmp_data/spark-4fa5dc13-0dd2-4ab5-8c0f-4e/tmpit1vktrl/model, flavor: spark). Fall back to return ['pyspark==4.0.0']. Set logging level to DEBUG to see the full traceback. \n\n2025/11/23 19:30:19 WARNING mlflow.models.model: Failed to validate serving input example {\n  \"inputs\": {\n    \"features_0\": 0.0,\n    \"features_1\": 0.0,\n    \"features_2\": 0.0,\n    \"features_3\": 0.0,\n    \"features_4\": 0.0,\n    \"features_5\": 0.495243504188556,\n    \"features_6\": 0.09683373562402385,\n    \"features_7\": 0.34374556297032516,\n    \"features_8\": 0.2866676132329973,\n    \"features_9\": 0.3448814425670879,\n    \"features_10\": 0.43944341899758627,\n    \"features_11\": 0.2902172369728809,\n    \"features_12\": 0.3989777083629135,\n    \"features_13\": 0.39542808462303,\n    \"features_14\": 0.20914383075394008,\n    \"features_15\": 0.2161010932841119\n  }\n}. Alternatively, you can avoid passing input example and pass model signature instead when logging the model. To ensure the input example is valid prior to serving, please try calling `mlflow.models.validate_serving_input` on the model uri and serving input example. A serving input example can be generated from model input example using `mlflow.models.convert_input_example_to_serving_input` function.\nGot error: UC volume path must be provided to save, log or load SparkML models in Databricks shared or serverless clusters. Specify environment variable 'MLFLOW_DFS_TMP' or 'dfs_tmpdir' argument that uses a UC volume path starting with '/Volumes/...' when saving, logging or loading a model.\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afce7fcfa6b8417b86e1e8db10965dd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r 65%|██████▌   | 13/20 [03:45<01:58, 16.90s/trial, best loss: -1.0]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.000794 seconds\n\nINFO:hyperopt.tpe:TPE using 13/13 trials with best loss -1.000000\n\n2025/11/23 19:30:33 WARNING mlflow.utils.requirements_utils: Found pyspark version (4.0.0+databricks.connect.17.2.2) contains a local version label (+databricks.connect.17.2.2). MLflow logged a pip requirement for this package as 'pyspark==4.0.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n\n2025/11/23 19:30:35 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /local_disk0/user_tmp_data/spark-4fa5dc13-0dd2-4ab5-8c0f-4e/tmpohx8a1nw/model, flavor: spark). Fall back to return ['pyspark==4.0.0']. Set logging level to DEBUG to see the full traceback. \n\n2025/11/23 19:30:36 WARNING mlflow.models.model: Failed to validate serving input example {\n  \"inputs\": {\n    \"features_0\": 0.0,\n    \"features_1\": 0.0,\n    \"features_2\": 0.0,\n    \"features_3\": 0.0,\n    \"features_4\": 0.0,\n    \"features_5\": 0.495243504188556,\n    \"features_6\": 0.09683373562402385,\n    \"features_7\": 0.34374556297032516,\n    \"features_8\": 0.2866676132329973,\n    \"features_9\": 0.3448814425670879,\n    \"features_10\": 0.43944341899758627,\n    \"features_11\": 0.2902172369728809,\n    \"features_12\": 0.3989777083629135,\n    \"features_13\": 0.39542808462303,\n    \"features_14\": 0.20914383075394008,\n    \"features_15\": 0.2161010932841119\n  }\n}. Alternatively, you can avoid passing input example and pass model signature instead when logging the model. To ensure the input example is valid prior to serving, please try calling `mlflow.models.validate_serving_input` on the model uri and serving input example. A serving input example can be generated from model input example using `mlflow.models.convert_input_example_to_serving_input` function.\nGot error: UC volume path must be provided to save, log or load SparkML models in Databricks shared or serverless clusters. Specify environment variable 'MLFLOW_DFS_TMP' or 'dfs_tmpdir' argument that uses a UC volume path starting with '/Volumes/...' when saving, logging or loading a model.\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf4d36ce62f54b4a98d3206b4f41f6ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r 70%|███████   | 14/20 [04:03<01:44, 17.38s/trial, best loss: -1.0]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.000677 seconds\n\nINFO:hyperopt.tpe:TPE using 14/14 trials with best loss -1.000000\n\n2025/11/23 19:30:52 WARNING mlflow.utils.requirements_utils: Found pyspark version (4.0.0+databricks.connect.17.2.2) contains a local version label (+databricks.connect.17.2.2). MLflow logged a pip requirement for this package as 'pyspark==4.0.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n\n2025/11/23 19:30:54 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /local_disk0/user_tmp_data/spark-4fa5dc13-0dd2-4ab5-8c0f-4e/tmpt254yows/model, flavor: spark). Fall back to return ['pyspark==4.0.0']. Set logging level to DEBUG to see the full traceback. \n\n2025/11/23 19:30:55 WARNING mlflow.models.model: Failed to validate serving input example {\n  \"inputs\": {\n    \"features_0\": 0.0,\n    \"features_1\": 0.0,\n    \"features_2\": 0.0,\n    \"features_3\": 0.0,\n    \"features_4\": 0.0,\n    \"features_5\": 0.495243504188556,\n    \"features_6\": 0.09683373562402385,\n    \"features_7\": 0.34374556297032516,\n    \"features_8\": 0.2866676132329973,\n    \"features_9\": 0.3448814425670879,\n    \"features_10\": 0.43944341899758627,\n    \"features_11\": 0.2902172369728809,\n    \"features_12\": 0.3989777083629135,\n    \"features_13\": 0.39542808462303,\n    \"features_14\": 0.20914383075394008,\n    \"features_15\": 0.2161010932841119\n  }\n}. Alternatively, you can avoid passing input example and pass model signature instead when logging the model. To ensure the input example is valid prior to serving, please try calling `mlflow.models.validate_serving_input` on the model uri and serving input example. A serving input example can be generated from model input example using `mlflow.models.convert_input_example_to_serving_input` function.\nGot error: UC volume path must be provided to save, log or load SparkML models in Databricks shared or serverless clusters. Specify environment variable 'MLFLOW_DFS_TMP' or 'dfs_tmpdir' argument that uses a UC volume path starting with '/Volumes/...' when saving, logging or loading a model.\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "841c7de3825b4c928b2b036320fb7550",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r 75%|███████▌  | 15/20 [04:20<01:25, 17.05s/trial, best loss: -1.0]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.000673 seconds\n\nINFO:hyperopt.tpe:TPE using 15/15 trials with best loss -1.000000\n\n2025/11/23 19:31:07 WARNING mlflow.utils.requirements_utils: Found pyspark version (4.0.0+databricks.connect.17.2.2) contains a local version label (+databricks.connect.17.2.2). MLflow logged a pip requirement for this package as 'pyspark==4.0.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n\n2025/11/23 19:31:10 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /local_disk0/user_tmp_data/spark-4fa5dc13-0dd2-4ab5-8c0f-4e/tmpuvvq499v/model, flavor: spark). Fall back to return ['pyspark==4.0.0']. Set logging level to DEBUG to see the full traceback. \n\n2025/11/23 19:31:10 WARNING mlflow.models.model: Failed to validate serving input example {\n  \"inputs\": {\n    \"features_0\": 0.0,\n    \"features_1\": 0.0,\n    \"features_2\": 0.0,\n    \"features_3\": 0.0,\n    \"features_4\": 0.0,\n    \"features_5\": 0.495243504188556,\n    \"features_6\": 0.09683373562402385,\n    \"features_7\": 0.34374556297032516,\n    \"features_8\": 0.2866676132329973,\n    \"features_9\": 0.3448814425670879,\n    \"features_10\": 0.43944341899758627,\n    \"features_11\": 0.2902172369728809,\n    \"features_12\": 0.3989777083629135,\n    \"features_13\": 0.39542808462303,\n    \"features_14\": 0.20914383075394008,\n    \"features_15\": 0.2161010932841119\n  }\n}. Alternatively, you can avoid passing input example and pass model signature instead when logging the model. To ensure the input example is valid prior to serving, please try calling `mlflow.models.validate_serving_input` on the model uri and serving input example. A serving input example can be generated from model input example using `mlflow.models.convert_input_example_to_serving_input` function.\nGot error: UC volume path must be provided to save, log or load SparkML models in Databricks shared or serverless clusters. Specify environment variable 'MLFLOW_DFS_TMP' or 'dfs_tmpdir' argument that uses a UC volume path starting with '/Volumes/...' when saving, logging or loading a model.\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63147df753704c598a081e7ea5404d49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r 80%|████████  | 16/20 [04:34<01:05, 16.33s/trial, best loss: -1.0]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.000705 seconds\n\nINFO:hyperopt.tpe:TPE using 16/16 trials with best loss -1.000000\n\n2025/11/23 19:31:23 WARNING mlflow.utils.requirements_utils: Found pyspark version (4.0.0+databricks.connect.17.2.2) contains a local version label (+databricks.connect.17.2.2). MLflow logged a pip requirement for this package as 'pyspark==4.0.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n\n2025/11/23 19:31:26 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /local_disk0/user_tmp_data/spark-4fa5dc13-0dd2-4ab5-8c0f-4e/tmpmm149yxo/model, flavor: spark). Fall back to return ['pyspark==4.0.0']. Set logging level to DEBUG to see the full traceback. \n\n2025/11/23 19:31:26 WARNING mlflow.models.model: Failed to validate serving input example {\n  \"inputs\": {\n    \"features_0\": 0.0,\n    \"features_1\": 0.0,\n    \"features_2\": 0.0,\n    \"features_3\": 0.0,\n    \"features_4\": 0.0,\n    \"features_5\": 0.495243504188556,\n    \"features_6\": 0.09683373562402385,\n    \"features_7\": 0.34374556297032516,\n    \"features_8\": 0.2866676132329973,\n    \"features_9\": 0.3448814425670879,\n    \"features_10\": 0.43944341899758627,\n    \"features_11\": 0.2902172369728809,\n    \"features_12\": 0.3989777083629135,\n    \"features_13\": 0.39542808462303,\n    \"features_14\": 0.20914383075394008,\n    \"features_15\": 0.2161010932841119\n  }\n}. Alternatively, you can avoid passing input example and pass model signature instead when logging the model. To ensure the input example is valid prior to serving, please try calling `mlflow.models.validate_serving_input` on the model uri and serving input example. A serving input example can be generated from model input example using `mlflow.models.convert_input_example_to_serving_input` function.\nGot error: UC volume path must be provided to save, log or load SparkML models in Databricks shared or serverless clusters. Specify environment variable 'MLFLOW_DFS_TMP' or 'dfs_tmpdir' argument that uses a UC volume path starting with '/Volumes/...' when saving, logging or loading a model.\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d094a2ab30f44170a7627e7558fa4f66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r 85%|████████▌ | 17/20 [04:51<00:49, 16.56s/trial, best loss: -1.0]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.000929 seconds\n\nINFO:hyperopt.tpe:TPE using 17/17 trials with best loss -1.000000\n\n2025/11/23 19:31:40 WARNING mlflow.utils.requirements_utils: Found pyspark version (4.0.0+databricks.connect.17.2.2) contains a local version label (+databricks.connect.17.2.2). MLflow logged a pip requirement for this package as 'pyspark==4.0.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n\n2025/11/23 19:31:43 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /local_disk0/user_tmp_data/spark-4fa5dc13-0dd2-4ab5-8c0f-4e/tmpfonbglb4/model, flavor: spark). Fall back to return ['pyspark==4.0.0']. Set logging level to DEBUG to see the full traceback. \n\n2025/11/23 19:31:43 WARNING mlflow.models.model: Failed to validate serving input example {\n  \"inputs\": {\n    \"features_0\": 0.0,\n    \"features_1\": 0.0,\n    \"features_2\": 0.0,\n    \"features_3\": 0.0,\n    \"features_4\": 0.0,\n    \"features_5\": 0.495243504188556,\n    \"features_6\": 0.09683373562402385,\n    \"features_7\": 0.34374556297032516,\n    \"features_8\": 0.2866676132329973,\n    \"features_9\": 0.3448814425670879,\n    \"features_10\": 0.43944341899758627,\n    \"features_11\": 0.2902172369728809,\n    \"features_12\": 0.3989777083629135,\n    \"features_13\": 0.39542808462303,\n    \"features_14\": 0.20914383075394008,\n    \"features_15\": 0.2161010932841119\n  }\n}. Alternatively, you can avoid passing input example and pass model signature instead when logging the model. To ensure the input example is valid prior to serving, please try calling `mlflow.models.validate_serving_input` on the model uri and serving input example. A serving input example can be generated from model input example using `mlflow.models.convert_input_example_to_serving_input` function.\nGot error: UC volume path must be provided to save, log or load SparkML models in Databricks shared or serverless clusters. Specify environment variable 'MLFLOW_DFS_TMP' or 'dfs_tmpdir' argument that uses a UC volume path starting with '/Volumes/...' when saving, logging or loading a model.\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a532dc4b99cc42928c9491d9c405ac1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r 90%|█████████ | 18/20 [05:09<00:33, 16.80s/trial, best loss: -1.0]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.000653 seconds\n\nINFO:hyperopt.tpe:TPE using 18/18 trials with best loss -1.000000\n\n2025/11/23 19:31:59 WARNING mlflow.utils.requirements_utils: Found pyspark version (4.0.0+databricks.connect.17.2.2) contains a local version label (+databricks.connect.17.2.2). MLflow logged a pip requirement for this package as 'pyspark==4.0.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n\n2025/11/23 19:32:02 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /local_disk0/user_tmp_data/spark-4fa5dc13-0dd2-4ab5-8c0f-4e/tmpd23lp3qr/model, flavor: spark). Fall back to return ['pyspark==4.0.0']. Set logging level to DEBUG to see the full traceback. \n\n2025/11/23 19:32:02 WARNING mlflow.models.model: Failed to validate serving input example {\n  \"inputs\": {\n    \"features_0\": 0.0,\n    \"features_1\": 0.0,\n    \"features_2\": 0.0,\n    \"features_3\": 0.0,\n    \"features_4\": 0.0,\n    \"features_5\": 0.495243504188556,\n    \"features_6\": 0.09683373562402385,\n    \"features_7\": 0.34374556297032516,\n    \"features_8\": 0.2866676132329973,\n    \"features_9\": 0.3448814425670879,\n    \"features_10\": 0.43944341899758627,\n    \"features_11\": 0.2902172369728809,\n    \"features_12\": 0.3989777083629135,\n    \"features_13\": 0.39542808462303,\n    \"features_14\": 0.20914383075394008,\n    \"features_15\": 0.2161010932841119\n  }\n}. Alternatively, you can avoid passing input example and pass model signature instead when logging the model. To ensure the input example is valid prior to serving, please try calling `mlflow.models.validate_serving_input` on the model uri and serving input example. A serving input example can be generated from model input example using `mlflow.models.convert_input_example_to_serving_input` function.\nGot error: UC volume path must be provided to save, log or load SparkML models in Databricks shared or serverless clusters. Specify environment variable 'MLFLOW_DFS_TMP' or 'dfs_tmpdir' argument that uses a UC volume path starting with '/Volumes/...' when saving, logging or loading a model.\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d05b67137cd340e19f8fe1397b50df14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r 95%|█████████▌| 19/20 [05:27<00:17, 17.19s/trial, best loss: -1.0]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.000738 seconds\n\nINFO:hyperopt.tpe:TPE using 19/19 trials with best loss -1.000000\n\n2025/11/23 19:32:16 WARNING mlflow.utils.requirements_utils: Found pyspark version (4.0.0+databricks.connect.17.2.2) contains a local version label (+databricks.connect.17.2.2). MLflow logged a pip requirement for this package as 'pyspark==4.0.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n\n2025/11/23 19:32:18 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /local_disk0/user_tmp_data/spark-4fa5dc13-0dd2-4ab5-8c0f-4e/tmpexgug43w/model, flavor: spark). Fall back to return ['pyspark==4.0.0']. Set logging level to DEBUG to see the full traceback. \n\n2025/11/23 19:32:18 WARNING mlflow.models.model: Failed to validate serving input example {\n  \"inputs\": {\n    \"features_0\": 0.0,\n    \"features_1\": 0.0,\n    \"features_2\": 0.0,\n    \"features_3\": 0.0,\n    \"features_4\": 0.0,\n    \"features_5\": 0.495243504188556,\n    \"features_6\": 0.09683373562402385,\n    \"features_7\": 0.34374556297032516,\n    \"features_8\": 0.2866676132329973,\n    \"features_9\": 0.3448814425670879,\n    \"features_10\": 0.43944341899758627,\n    \"features_11\": 0.2902172369728809,\n    \"features_12\": 0.3989777083629135,\n    \"features_13\": 0.39542808462303,\n    \"features_14\": 0.20914383075394008,\n    \"features_15\": 0.2161010932841119\n  }\n}. Alternatively, you can avoid passing input example and pass model signature instead when logging the model. To ensure the input example is valid prior to serving, please try calling `mlflow.models.validate_serving_input` on the model uri and serving input example. A serving input example can be generated from model input example using `mlflow.models.convert_input_example_to_serving_input` function.\nGot error: UC volume path must be provided to save, log or load SparkML models in Databricks shared or serverless clusters. Specify environment variable 'MLFLOW_DFS_TMP' or 'dfs_tmpdir' argument that uses a UC volume path starting with '/Volumes/...' when saving, logging or loading a model.\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7de7b46c1c64a29a9db88b1f8cb6ef3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r100%|██████████| 20/20 [05:45<00:00, 17.56s/trial, best loss: -1.0]\r100%|██████████| 20/20 [05:45<00:00, 17.28s/trial, best loss: -1.0]\n"
     ]
    }
   ],
   "source": [
    "trials_rf = Trials()\n",
    "\n",
    "#Aplicación y registro de entrenamiento de modelo con Hyperopt\n",
    "best_rf = fmin(\n",
    "fn=objectivo_rf,\n",
    "space=space_rf,\n",
    "algo=tpe.suggest,\n",
    "max_evals=20,\n",
    "trials=trials_rf\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd31949f-3389-456d-938a-7319e9543a2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Selección y Registro de Random Forest\n",
    "\n",
    "Se selecciona el modelo con mejor resultado en las ejecuciones con diferentes hiperparametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "025f5697-0963-4caf-bf10-1d234c1cd68e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully registered model 'workspace.default.churn_rf_model'.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "936b020be7fd4947869e2b23511f531f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06c3586a58db439d893ecd6928dbb9c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Random Forest best model registered: <ModelVersion: aliases=[], creation_timestamp=1763926346653, current_stage=None, description='', last_updated_timestamp=1763926350113, name='workspace.default.churn_rf_model', run_id='2fa6be48a44141cfa97005e3d38bc203', run_link=None, source='dbfs:/databricks/mlflow-tracking/2671492619073786/2fa6be48a44141cfa97005e3d38bc203/artifacts/RF_Model', status='READY', status_message='', tags={}, user_id='josestunt95@gmail.com', version='1'>\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '1' of model 'workspace.default.churn_rf_model'.\n"
     ]
    }
   ],
   "source": [
    "#Consulta MLflow para seleccionar la mejor RF por AUC como base\n",
    "best_rf_run_info = mlflow.search_runs(\n",
    "filter_string=\"tags.mlflow.runName = 'RF_Hyperopt'\",\n",
    "order_by=[\"metrics.AUC DESC\"],\n",
    "max_results=1\n",
    ").iloc[0]\n",
    "\n",
    "\n",
    "best_rf_run_id = best_rf_run_info.run_id\n",
    "rf_model_uri = f\"runs:/{best_rf_run_id}/RF_Model\"\n",
    "\n",
    "\n",
    "rf_registered = mlflow.register_model(\n",
    "model_uri=rf_model_uri,\n",
    "name=\"Churn_RF_Model\"\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Random Forest model registrado:\", rf_registered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f3533567-68ba-4632-b389-eca26c920b35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Conclución\n",
    "\n",
    "Se registran dos modelos, regresión logistica y Random forest. Por temas de tiempo no itere demasiadas ocasiones o revise mas en profundidad la perfección del modelo; tuve enfoque en construir el flujo de trabajo completo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab0c435f-1197-4bd9-89ea-2c0868eeb13a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Uso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc66b362-e357-4860-8942-8db7bc461ed7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.12/site-packages/databricks/sdk/errors/base.py:87: UserWarning: The 'retry_after_secs' parameter of DatabricksError is deprecated and will be removed in a future version.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "616d5b4972c3403894334a523be7fa6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mMlflowException\u001B[0m                           Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-8963420621813008>, line 4\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmlflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mspark\u001B[39;00m\n",
       "\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# Cargar modelos desde Model Registry\u001B[39;00m\n",
       "\u001B[0;32m----> 4\u001B[0m lr_model \u001B[38;5;241m=\u001B[39m mlflow\u001B[38;5;241m.\u001B[39mspark\u001B[38;5;241m.\u001B[39mload_model(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodels:/churn_lr_model/1\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m      5\u001B[0m rf_model \u001B[38;5;241m=\u001B[39m mlflow\u001B[38;5;241m.\u001B[39mspark\u001B[38;5;241m.\u001B[39mload_model(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodels:/churn_rf_model/1\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/mlflow/spark/__init__.py:985\u001B[0m, in \u001B[0;36mload_model\u001B[0;34m(model_uri, dfs_tmpdir, dst_path)\u001B[0m\n",
       "\u001B[1;32m    983\u001B[0m sparkml_model_uri \u001B[38;5;241m=\u001B[39m append_to_uri_path(model_uri, flavor_conf[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel_data\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n",
       "\u001B[1;32m    984\u001B[0m local_sparkml_model_path \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(local_mlflow_model_path, flavor_conf[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel_data\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n",
       "\u001B[0;32m--> 985\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _load_model(\n",
       "\u001B[1;32m    986\u001B[0m     model_uri\u001B[38;5;241m=\u001B[39msparkml_model_uri,\n",
       "\u001B[1;32m    987\u001B[0m     dfs_tmpdir_base\u001B[38;5;241m=\u001B[39mdfs_tmpdir,\n",
       "\u001B[1;32m    988\u001B[0m     local_model_path\u001B[38;5;241m=\u001B[39mlocal_sparkml_model_path,\n",
       "\u001B[1;32m    989\u001B[0m )\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/mlflow/spark/__init__.py:896\u001B[0m, in \u001B[0;36m_load_model\u001B[0;34m(model_uri, dfs_tmpdir_base, local_model_path)\u001B[0m\n",
       "\u001B[1;32m    892\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mml\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpipeline\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m PipelineModel\n",
       "\u001B[1;32m    894\u001B[0m dfs_tmpdir \u001B[38;5;241m=\u001B[39m generate_tmp_dfs_path(dfs_tmpdir_base \u001B[38;5;129;01mor\u001B[39;00m MLFLOW_DFS_TMP\u001B[38;5;241m.\u001B[39mget())\n",
       "\u001B[0;32m--> 896\u001B[0m _check_databricks_uc_volume_tmpdir_availability(dfs_tmpdir)\n",
       "\u001B[1;32m    897\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n",
       "\u001B[1;32m    898\u001B[0m     databricks_utils\u001B[38;5;241m.\u001B[39mis_in_databricks_serverless_runtime()\n",
       "\u001B[1;32m    899\u001B[0m     \u001B[38;5;129;01mor\u001B[39;00m databricks_utils\u001B[38;5;241m.\u001B[39mis_in_databricks_shared_cluster_runtime()\n",
       "\u001B[1;32m    900\u001B[0m ):\n",
       "\u001B[1;32m    901\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _load_model_databricks_uc_volume(\n",
       "\u001B[1;32m    902\u001B[0m         dfs_tmpdir, local_model_path \u001B[38;5;129;01mor\u001B[39;00m _download_artifact_from_uri(model_uri)\n",
       "\u001B[1;32m    903\u001B[0m     )\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/mlflow/spark/__init__.py:712\u001B[0m, in \u001B[0;36m_check_databricks_uc_volume_tmpdir_availability\u001B[0;34m(dfs_tmpdir)\u001B[0m\n",
       "\u001B[1;32m    707\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n",
       "\u001B[1;32m    708\u001B[0m     databricks_utils\u001B[38;5;241m.\u001B[39mis_in_databricks_serverless_runtime()\n",
       "\u001B[1;32m    709\u001B[0m     \u001B[38;5;129;01mor\u001B[39;00m databricks_utils\u001B[38;5;241m.\u001B[39mis_in_databricks_shared_cluster_runtime()\n",
       "\u001B[1;32m    710\u001B[0m ):\n",
       "\u001B[1;32m    711\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m dfs_tmpdir \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m _is_uc_volume_uri(dfs_tmpdir):\n",
       "\u001B[0;32m--> 712\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m MlflowException(\n",
       "\u001B[1;32m    713\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUC volume path must be provided to save, log or load SparkML models \u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    714\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124min Databricks shared or serverless clusters. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    715\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSpecify environment variable \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMLFLOW_DFS_TMP\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    716\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mor \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdfs_tmpdir\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m argument that uses a UC volume path starting with \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m/Volumes/...\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    717\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwhen saving, logging or loading a model.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    718\u001B[0m         )\n",
       "\n",
       "\u001B[0;31mMlflowException\u001B[0m: UC volume path must be provided to save, log or load SparkML models in Databricks shared or serverless clusters. Specify environment variable 'MLFLOW_DFS_TMP' or 'dfs_tmpdir' argument that uses a UC volume path starting with '/Volumes/...' when saving, logging or loading a model."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "MlflowException",
        "evalue": "UC volume path must be provided to save, log or load SparkML models in Databricks shared or serverless clusters. Specify environment variable 'MLFLOW_DFS_TMP' or 'dfs_tmpdir' argument that uses a UC volume path starting with '/Volumes/...' when saving, logging or loading a model."
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>MlflowException</span>: UC volume path must be provided to save, log or load SparkML models in Databricks shared or serverless clusters. Specify environment variable 'MLFLOW_DFS_TMP' or 'dfs_tmpdir' argument that uses a UC volume path starting with '/Volumes/...' when saving, logging or loading a model.\n[Trace ID: 00-4b600dc20ba49b9f3f5867c334d73a31-3267ca9e0c12d1b7-00]"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mMlflowException\u001B[0m                           Traceback (most recent call last)",
        "File \u001B[0;32m<command-8963420621813008>, line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmlflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mspark\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# Cargar modelos desde Model Registry\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m lr_model \u001B[38;5;241m=\u001B[39m mlflow\u001B[38;5;241m.\u001B[39mspark\u001B[38;5;241m.\u001B[39mload_model(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodels:/churn_lr_model/1\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      5\u001B[0m rf_model \u001B[38;5;241m=\u001B[39m mlflow\u001B[38;5;241m.\u001B[39mspark\u001B[38;5;241m.\u001B[39mload_model(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodels:/churn_rf_model/1\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/mlflow/spark/__init__.py:985\u001B[0m, in \u001B[0;36mload_model\u001B[0;34m(model_uri, dfs_tmpdir, dst_path)\u001B[0m\n\u001B[1;32m    983\u001B[0m sparkml_model_uri \u001B[38;5;241m=\u001B[39m append_to_uri_path(model_uri, flavor_conf[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel_data\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[1;32m    984\u001B[0m local_sparkml_model_path \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(local_mlflow_model_path, flavor_conf[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel_data\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[0;32m--> 985\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _load_model(\n\u001B[1;32m    986\u001B[0m     model_uri\u001B[38;5;241m=\u001B[39msparkml_model_uri,\n\u001B[1;32m    987\u001B[0m     dfs_tmpdir_base\u001B[38;5;241m=\u001B[39mdfs_tmpdir,\n\u001B[1;32m    988\u001B[0m     local_model_path\u001B[38;5;241m=\u001B[39mlocal_sparkml_model_path,\n\u001B[1;32m    989\u001B[0m )\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/mlflow/spark/__init__.py:896\u001B[0m, in \u001B[0;36m_load_model\u001B[0;34m(model_uri, dfs_tmpdir_base, local_model_path)\u001B[0m\n\u001B[1;32m    892\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mml\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpipeline\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m PipelineModel\n\u001B[1;32m    894\u001B[0m dfs_tmpdir \u001B[38;5;241m=\u001B[39m generate_tmp_dfs_path(dfs_tmpdir_base \u001B[38;5;129;01mor\u001B[39;00m MLFLOW_DFS_TMP\u001B[38;5;241m.\u001B[39mget())\n\u001B[0;32m--> 896\u001B[0m _check_databricks_uc_volume_tmpdir_availability(dfs_tmpdir)\n\u001B[1;32m    897\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m    898\u001B[0m     databricks_utils\u001B[38;5;241m.\u001B[39mis_in_databricks_serverless_runtime()\n\u001B[1;32m    899\u001B[0m     \u001B[38;5;129;01mor\u001B[39;00m databricks_utils\u001B[38;5;241m.\u001B[39mis_in_databricks_shared_cluster_runtime()\n\u001B[1;32m    900\u001B[0m ):\n\u001B[1;32m    901\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _load_model_databricks_uc_volume(\n\u001B[1;32m    902\u001B[0m         dfs_tmpdir, local_model_path \u001B[38;5;129;01mor\u001B[39;00m _download_artifact_from_uri(model_uri)\n\u001B[1;32m    903\u001B[0m     )\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/mlflow/spark/__init__.py:712\u001B[0m, in \u001B[0;36m_check_databricks_uc_volume_tmpdir_availability\u001B[0;34m(dfs_tmpdir)\u001B[0m\n\u001B[1;32m    707\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m    708\u001B[0m     databricks_utils\u001B[38;5;241m.\u001B[39mis_in_databricks_serverless_runtime()\n\u001B[1;32m    709\u001B[0m     \u001B[38;5;129;01mor\u001B[39;00m databricks_utils\u001B[38;5;241m.\u001B[39mis_in_databricks_shared_cluster_runtime()\n\u001B[1;32m    710\u001B[0m ):\n\u001B[1;32m    711\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m dfs_tmpdir \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m _is_uc_volume_uri(dfs_tmpdir):\n\u001B[0;32m--> 712\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m MlflowException(\n\u001B[1;32m    713\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUC volume path must be provided to save, log or load SparkML models \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    714\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124min Databricks shared or serverless clusters. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    715\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSpecify environment variable \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMLFLOW_DFS_TMP\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    716\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mor \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdfs_tmpdir\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m argument that uses a UC volume path starting with \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m/Volumes/...\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    717\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwhen saving, logging or loading a model.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    718\u001B[0m         )\n",
        "\u001B[0;31mMlflowException\u001B[0m: UC volume path must be provided to save, log or load SparkML models in Databricks shared or serverless clusters. Specify environment variable 'MLFLOW_DFS_TMP' or 'dfs_tmpdir' argument that uses a UC volume path starting with '/Volumes/...' when saving, logging or loading a model."
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import mlflow.spark\n",
    "\n",
    "# Cargar modelos desde Model Registry\n",
    "lr_model = mlflow.spark.load_model(\"models:/churn_lr_model/1\")\n",
    "rf_model = mlflow.spark.load_model(\"models:/churn_rf_model/1\")\n",
    "\n",
    "\n",
    "# Ejemplo de predicción\n",
    "# predictions_lr = lr_model.transform(spark_df)\n",
    "# predictions_rf = rf_model.transform(spark_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc299446-7963-417e-a1e3-d2fe0dfddcd4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.12/site-packages/databricks/sdk/errors/base.py:87: UserWarning: The 'retry_after_secs' parameter of DatabricksError is deprecated and will be removed in a future version.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed02ace292164aaaa9904602c0db8e68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d87724fbf5b4dea8c46fbc3fff39f0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "#Cargar modelos desde Model Registry usando dfs_tmpdir en un volumen creado en el volumen corona en el shema gold, volumen models\n",
    "lr_model = mlflow.spark.load_model(\n",
    "    \"models:/churn_lr_model/1\",\n",
    "    dfs_tmpdir=\"/Volumes/corona/gold/models/mlflow_tmp\"\n",
    ")\n",
    "rf_model = mlflow.spark.load_model(\n",
    "    \"models:/churn_rf_model/1\",\n",
    "    dfs_tmpdir=\"/Volumes/corona/gold/models/mlflow_tmp\"\n",
    ")\n",
    "\n",
    "# Ejemplo de predicción\n",
    "# predictions_lr = lr_model.transform(spark_df)\n",
    "# predictions_rf = rf_model.transform(spark_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6db8d677-1068-4a29-8b15-613dbb29cb99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Simular datos nuevos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7d15cc1-9785-4cfb-bc25-c80a477624ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "nuevo_cliente_features = [\n",
    "    1.0,   #Partner (Yes)\n",
    "    0.0,   #Dependents (No)\n",
    "    1.0,   #PhoneService (Yes)\n",
    "    1.0,   #PaperlessBilling (Yes)\n",
    "    0.0,   #Churn placeholder (no se usa)\n",
    "    0.49,  #gender_freq\n",
    "    0.33,  #MultipleLines_freq\n",
    "    0.41,  #InternetService_freq\n",
    "    0.28,  #OnlineSecurity_freq\n",
    "    0.43,  #OnlineBackup_freq\n",
    "    0.35,  #DeviceProtection_freq\n",
    "    0.48,  #TechSupport_freq\n",
    "    0.40,  #StreamingTV_freq\n",
    "    0.39,  #StreamingMovies_freq\n",
    "    0.55,  #Contract_freq\n",
    "    0.33   #PaymentMethod_freq\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a422e28f-3a47-47f5-a37f-2cd7243a2d36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#vector de entrada de los nuevos datos\n",
    "vector_input = Vectors.dense(nuevo_cliente_features)\n",
    "\n",
    "#construcción del dataframe de prueba\n",
    "df_inferencia = spark.createDataFrame(\n",
    "    [Row(features=vector_input)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96b35624-656c-4f6e-b497-56d8182f55de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------> Regresión Logistica\n+----------+----------------------------------------+\n|prediction|probability                             |\n+----------+----------------------------------------+\n|0.0       |[0.8322375068575825,0.16776249314241753]|\n+----------+----------------------------------------+\n\n---------> Random Forest\n+----------+----------------------------------------+\n|prediction|probability                             |\n+----------+----------------------------------------+\n|0.0       |[0.9166511469139784,0.08334885308602145]|\n+----------+----------------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "pred_lr = lr_model.transform(df_inferencia)\n",
    "print(\"---------> Regresión Logistica\")\n",
    "pred_lr.select(\"prediction\", \"probability\").show(truncate=False)\n",
    "\n",
    "pred_rf = rf_model.transform(df_inferencia)\n",
    "print(\"---------> Random Forest\")\n",
    "pred_rf.select(\"prediction\", \"probability\").show(truncate=False)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_entrenamiento_tracking_y_testeo",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}